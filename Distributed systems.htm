<!DOCTYPE html>
<!-- saved from url=(0040)https://www.printfriendly.com/p/g/ahaWWn -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script type="text/javascript" async="" src="./Distributed systems_files/analytics.js"></script><script async="" src="./Distributed systems_files/apstag.js"></script><script async="" type="text/javascript" src="./Distributed systems_files/gpt.js"></script><meta content="IE=edge" http-equiv="X-UA-Compatible"><meta content="upgrade-insecure-requests" http-equiv="Content-Security-Policy"><meta content="width=device-width, initial-scale=1" name="viewport"><meta content="" name="description"><link href="https://app.printfriendly.com/assets/site/favicon-3738de9b6a75de4cbca47dae36fa832697dba6226c616ecc65d29d3c4da5ecfb.ico" rel="SHORTCUT ICON"><title>Distributed systems</title><script async="" src="./Distributed systems_files/gtm.js"></script><script>dataLayer = [{'adType': 'publisher_group_2',
              'ad_typeReason': 'domain',
              'nsfw_keywords': '',
              'contentHost': 'book.mixu.net',
              'fetchedByAdsense': true,
              'source': 'other',
              'lang': 'en',
              'device':'desktop'}];</script>
  <script src="./Distributed systems_files/printfriendly.com.js" async=""></script>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5PHJ6G');</script>
<!-- End Google Tag Manager -->

<script async="async" src="./Distributed systems_files/gpt.js"></script><script>var googletag = googletag || {};
googletag.cmd = googletag.cmd || [];</script><script>window.deployads = window.deployads || [];</script><script>var doubleclickSlots =[];
googletag.cmd.push(function() {
  
  googletag.defineSlot('/21822432912/pfcom-box1', [[300, 250], [336, 280]], 'div-gpt-ad-1562914060901-0').addService(googletag.pubads());

  googletag.defineSlot('/21822432912/pfcom-box2', [[300, 250], [336, 280]], 'div-gpt-ad-1574251754981-0').addService(googletag.pubads());

  googletag.pubads().enableSingleRequest();
  googletag.pubads().collapseEmptyDivs();
  deployads.push(function () { deployads.gpt.pubadsDisableInitialLoad();deployads.gpt.enableServices(); });
});</script><script>var isMobileTablet = false
var userAgent = {
  browser: 'chrome',
  version: '81.0.4044.129',
  os: 'os x 10.13.6'
}</script><link href="https://chrome.google.com/webstore/detail/ohlencieiipommannpdfcmfdpjjmeolj" rel="chrome-webstore-item"><style type="text/css"></style><style type="text/css">
  #pf-content img.mediumImage {
      margin: 1em 0 1em 1.5em;
      clear: right;
      display: inline;
      float: right;
  }
</style>
<link rel="stylesheet" media="screen, print" href="./Distributed systems_files/main-4d0e2b95c432d947217d9cc9882dbbc005e0e8c0c4e9c4195b553884cdf5f2f8.css"><meta content="noindex" name="robots"><meta content="nopin" description="Please pin from the original page" name="pinterest"><script>var Settings = {
  user: {"imageDisplayStyle":null,"disablePDF":false,"disablePrint":false,"disableEmail":false,"deleteEnabled":true,"imagesSize":"full-size","customCSSURL":null,"headerImageUrl":null,"headerTagline":null},
  content: {"maxWaitTimeInSeconds":40,"host":"book.mixu.net","showAds":true,"brandFree":false,"url":"http://book.mixu.net/distsys/ebook.html","originalUrl":"http://book.mixu.net/distsys/ebook.html","referer":"https://www.printfriendly.com/p/g/ahaWWn","source":"other","data":{"title":"Distributed systems","direction":"ltr","nsfw_state":"absent"}},
  androidDevice: false,
  request: {
    protocol: 'https://'
  },
  adType: ''
}
var startTime = 1588691950;</script><link rel="preload" href="./Distributed systems_files/f.txt" as="script"><script type="text/javascript" src="./Distributed systems_files/f.txt"></script><link rel="preload" href="./Distributed systems_files/f(1).txt" as="script"><script type="text/javascript" src="./Distributed systems_files/f(1).txt"></script><script src="./Distributed systems_files/pubads_impl_2020042703.js" async=""></script><script src="./Distributed systems_files/print-d25f461001619ef00d546f62e856443228190ff7c0ad5cc4de47bd6cdaf33f52.js"></script><link rel="stylesheet" media="screen, print" href="./Distributed systems_files/main-7cb3b8436b36ec7fd5127bdeed26968ef021486050128294fbdf5ee9130ef189.css"><link rel="stylesheet" media="screen, print" href="./Distributed systems_files/main-90285eae6877a6e25288a8946c40fc506bd8140a1a5cf75317045847639e7825.css"><link rel="dns-prefetch" href="https://adserver.adtechus.com/"><link rel="preconnect" href="https://adserver.adtechus.com/" crossorigin=""><link rel="dns-prefetch" href="https://ib.adnxs.com/"><link rel="preconnect" href="https://ib.adnxs.com/" crossorigin=""><iframe scrolling="no" marginwidth="0" marginheight="0" frameborder="0" src="./Distributed systems_files/multi-sync.html" style="width: 1px; height: 1px; border: 0px; vertical-align: bottom;"></iframe></head><body class="pf-app-body pfcom-site print-page-body source-referral" id="pfcom-print"><!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5PHJ6G"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) --><svg style="position: absolute; width: 0; height: 0; overflow: hidden;" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><symbol id="icon-reply" viewBox="0 0 32 32"><path d="M4.687 11.119l9.287 8.933v-5.412c2.813 0 9.973 0.062 9.973 7.426 0 3.855-2.734 7.072-6.369 7.816 5.842-0.792 10.359-5.747 10.359-11.806 0-11.256-12.026-11.352-13.963-11.352v-4.606l-9.287 9.001z"></path></symbol><symbol id="icon-download" viewBox="0 0 26 28"><path d="M20 21c0-0.547-0.453-1-1-1s-1 0.453-1 1 0.453 1 1 1 1-0.453 1-1zM24 21c0-0.547-0.453-1-1-1s-1 0.453-1 1 0.453 1 1 1 1-0.453 1-1zM26 17.5v5c0 0.828-0.672 1.5-1.5 1.5h-23c-0.828 0-1.5-0.672-1.5-1.5v-5c0-0.828 0.672-1.5 1.5-1.5h7.266l2.109 2.125c0.578 0.562 1.328 0.875 2.125 0.875s1.547-0.313 2.125-0.875l2.125-2.125h7.25c0.828 0 1.5 0.672 1.5 1.5zM20.922 8.609c0.156 0.375 0.078 0.812-0.219 1.094l-7 7c-0.187 0.203-0.453 0.297-0.703 0.297s-0.516-0.094-0.703-0.297l-7-7c-0.297-0.281-0.375-0.719-0.219-1.094 0.156-0.359 0.516-0.609 0.922-0.609h4v-7c0-0.547 0.453-1 1-1h4c0.547 0 1 0.453 1 1v7h4c0.406 0 0.766 0.25 0.922 0.609z"></path></symbol><symbol id="icon-text-height" viewBox="0 0 28 28"><path d="M27.25 22c0.688 0 0.906 0.438 0.484 0.984l-1.969 2.531c-0.422 0.547-1.109 0.547-1.531 0l-1.969-2.531c-0.422-0.547-0.203-0.984 0.484-0.984h1.25v-16h-1.25c-0.688 0-0.906-0.438-0.484-0.984l1.969-2.531c0.422-0.547 1.109-0.547 1.531 0l1.969 2.531c0.422 0.547 0.203 0.984-0.484 0.984h-1.25v16h1.25zM1.266 2.016l0.844 0.422c0.109 0.047 2.969 0.078 3.297 0.078 1.375 0 2.75-0.063 4.125-0.063 1.125 0 2.234 0.016 3.359 0.016h4.578c0.625 0 0.984 0.141 1.406-0.453l0.656-0.016c0.141 0 0.297 0.016 0.438 0.016 0.031 1.75 0.031 3.5 0.031 5.25 0 0.547 0.016 1.156-0.078 1.703-0.344 0.125-0.703 0.234-1.062 0.281-0.359-0.625-0.609-1.313-0.844-2-0.109-0.313-0.484-2.422-0.516-2.453-0.328-0.406-0.688-0.328-1.172-0.328-1.422 0-2.906-0.063-4.312 0.109-0.078 0.688-0.141 1.422-0.125 2.125 0.016 4.391 0.063 8.781 0.063 13.172 0 1.203-0.187 2.469 0.156 3.625 1.188 0.609 2.594 0.703 3.813 1.25 0.031 0.25 0.078 0.516 0.078 0.781 0 0.141-0.016 0.297-0.047 0.453l-0.531 0.016c-2.219 0.063-4.406-0.281-6.641-0.281-1.578 0-3.156 0.281-4.734 0.281-0.016-0.266-0.047-0.547-0.047-0.812v-0.141c0.594-0.953 2.734-0.969 3.719-1.547 0.344-0.766 0.297-5 0.297-5.984 0-3.156-0.094-6.312-0.094-9.469v-1.828c0-0.281 0.063-1.406-0.125-1.625-0.219-0.234-2.266-0.187-2.531-0.187-0.578 0-2.25 0.266-2.703 0.594-0.75 0.516-0.75 3.641-1.687 3.703-0.281-0.172-0.672-0.422-0.875-0.688v-5.984z"></path></symbol><symbol id="icon-image" viewBox="0 0 30 28"><path d="M10 9c0 1.656-1.344 3-3 3s-3-1.344-3-3 1.344-3 3-3 3 1.344 3 3zM26 15v7h-22v-3l5-5 2.5 2.5 8-8zM27.5 4h-25c-0.266 0-0.5 0.234-0.5 0.5v19c0 0.266 0.234 0.5 0.5 0.5h25c0.266 0 0.5-0.234 0.5-0.5v-19c0-0.266-0.234-0.5-0.5-0.5zM30 4.5v19c0 1.375-1.125 2.5-2.5 2.5h-25c-1.375 0-2.5-1.125-2.5-2.5v-19c0-1.375 1.125-2.5 2.5-2.5h25c1.375 0 2.5 1.125 2.5 2.5z"></path></symbol><symbol id="icon-trash" viewBox="0 0 22 28"><path d="M8 21.5v-11c0-0.281-0.219-0.5-0.5-0.5h-1c-0.281 0-0.5 0.219-0.5 0.5v11c0 0.281 0.219 0.5 0.5 0.5h1c0.281 0 0.5-0.219 0.5-0.5zM12 21.5v-11c0-0.281-0.219-0.5-0.5-0.5h-1c-0.281 0-0.5 0.219-0.5 0.5v11c0 0.281 0.219 0.5 0.5 0.5h1c0.281 0 0.5-0.219 0.5-0.5zM16 21.5v-11c0-0.281-0.219-0.5-0.5-0.5h-1c-0.281 0-0.5 0.219-0.5 0.5v11c0 0.281 0.219 0.5 0.5 0.5h1c0.281 0 0.5-0.219 0.5-0.5zM7.5 6h7l-0.75-1.828c-0.047-0.063-0.187-0.156-0.266-0.172h-4.953c-0.094 0.016-0.219 0.109-0.266 0.172zM22 6.5v1c0 0.281-0.219 0.5-0.5 0.5h-1.5v14.812c0 1.719-1.125 3.187-2.5 3.187h-13c-1.375 0-2.5-1.406-2.5-3.125v-14.875h-1.5c-0.281 0-0.5-0.219-0.5-0.5v-1c0-0.281 0.219-0.5 0.5-0.5h4.828l1.094-2.609c0.313-0.766 1.25-1.391 2.078-1.391h5c0.828 0 1.766 0.625 2.078 1.391l1.094 2.609h4.828c0.281 0 0.5 0.219 0.5 0.5z"></path></symbol><symbol id="icon-delete" viewBox="0 0 24 24"><path d="M18.984 3.984v2.016h-13.969v-2.016h3.469l1.031-0.984h4.969l1.031 0.984h3.469zM6 18.984v-12h12v12c0 1.078-0.938 2.016-2.016 2.016h-7.969c-1.078 0-2.016-0.938-2.016-2.016z"></path></symbol><symbol id="logo-printfriendly" viewBox="0 0 178 45"><g fill="none" fill-rule="evenodd"><text fill="#FFF" font-family="Roboto Slab" font-size="26" font-weight="700"><tspan x="0" y="28">p</tspan><tspan x="17" y="28">r</tspan><tspan x="30" y="28">i</tspan><tspan x="40" y="28">n</tspan><tspan x="57" y="28">t</tspan></text><text fill="#61D1D5" font-family="Roboto Slab" font-size="26" font-weight="700"><tspan x="68" y="28">f</tspan><tspan x="79" y="28">r</tspan><tspan x="92" y="28">i</tspan><tspan x="102" y="28">e</tspan><tspan x="117" y="28">n</tspan><tspan x="134" y="28">d</tspan><tspan x="152" y="28">l</tspan><tspan x="161" y="28">y</tspan></text><text fill="#FFF" font-family="Roboto-Medium, Roboto" font-size="10" font-weight="400"><tspan x="21" y="40">Save Money &amp; the Environment</tspan></text></g></symbol></defs></svg><div class="pf-app-container d-flex flex-column"><div class="container-fluid pf-masthead hide-for-print hide-for-referral"><div class="container"><div class="d-flex flex-row justify-content-between align-items-center flex-nowrap px-3"><a class="navbar-brand" href="https://www.printfriendly.com/" title="PrintFriendly &amp;amp; PDF"><svg class="logo-printfriendly"><use xlink:href="#logo-printfriendly"></use></svg></a><div class="nav-message" id="browser-extension-message-wrapper" style="display: block;"><div class="pfalert" id="browser-extension-message"><div class="d-none d-md-inline-block"><span class="navbar-text">Free &nbsp;</span><a id="browser-extension-more" target="_blank" href="https://chrome.google.com/webstore/detail/ohlencieiipommannpdfcmfdpjjmeolj">Browser Extension!</a></div><a class="btn btn-pf btn-success ml-2" id="browser-extension-button">Install PrintFriendly &amp; PDF</a></div></div></div></div></div><div class="container d-flex flex-column pf-app-wrapper justify-content-start"><div class="pf-header d-none d-sm-flex print-no"></div><div class="d-flex flex-row pf-app-row flex-auto"><div class="pf-app-inner d-flex flex-column pf-app-shadow"><div class="pf-toolbar d-flex flex-row print-no hide-for-print"><div class="pf-actions d-flex flex-row"><button class="btn-pf d-flex flex-sm-row flex-column align-items-center justify-content-center js-print-button px-lg-3 px-xl-4" id="w-print" type="button"><span class="pf-sprite"></span><strong class="ml-1" id="print">Print</strong></button><button class="btn-pf d-flex flex-sm-row flex-column align-items-center justify-content-center js-pdf-button px-lg-3 px-xl-4" id="w-pdf" type="button"><span class="pf-sprite"></span><strong class="small-caps ml-1" id="pdf">PDF</strong></button><button class="btn-pf d-flex flex-sm-row flex-column align-items-center justify-content-center js-email-button px-lg-3 px-xl-4" id="w-email" type="button"><span class="pf-sprite"></span><strong class="ml-1" id="email">Email</strong></button><div class="b-l-white"></div></div><div class="pf-edit d-flex flex-row justify-content-center"><div class="pf-text-size d-flex flex-row align-items-center mx-1"><svg class="icon icon-text-height mr-1 mr-md-2 d-none d-sm-inline-block"><use xlink:href="#icon-text-height"></use></svg><select class="form-control js-text-size" id="text-size" name="txtsize"><option value="pf-15">130%</option><option value="pf-14">120%</option><option value="pf-13">110%</option><option value="pf-12">100%</option><option value="pf-11">90%</option><option value="pf-10">80%</option><option value="pf-9">70%</option></select></div><div class="pf-image-size d-flex flex-sm-row align-items-center mx-2 mx-md-4"><svg class="icon icon-image mr-1 mr-md-2 d-none d-sm-inline-block"><use xlink:href="#icon-image"></use></svg><select class="form-control js-image-size" id="imagesize" name="imgsize"><option value="full-size">100%</option><option value="large">75%</option><option value="medium">50%</option><option value="small">25%</option><option value="remove-images">0</option></select></div><a class="btn-pf js-undo-button d-flex flex-row align-items-center mx-1" id="w-undo"><svg class="icon icon-reply fs-3"><use xlink:href="#icon-reply"></use></svg><span class="d-none d-sm-inline-block ml-1">Undo</span></a></div><button aria-label="Close" class="close btn-pf align-self-start px-2 js-app-close-button" id="pf-app-close" type="button"><span aria-hidden="true">×</span></button></div><div class="pf-dialog-mask js-dialog-frame hide-for-print px-0 px-sm-2 px-md-4" id="pf-dialog-frame"><div class="pf-dialog-container p-2 mx-auto my-4"><button aria-label="Close" class="close btn-pf align-self-start p-2 js-dialog-close-button fs-5" id="pf-dialog-close" type="button"><span aria-hidden="true" class="fs-5">×</span></button><div class="pf-dialog-header px-3 px-sm-5 pt-4 pb-1 rounded-top" data-js="pf-dialog"><div class="pt-3 pb-4" id="pf-dialog-print"><div class="pf-sprite mr-3 float-left"></div><h2>Printing Your Page</h2><p>Sending to your printer. <a onclick="window.print();return false;" href="https://www.printfriendly.com/p/g/ahaWWn#" class="btn btn-secondary btn-pf btn-sm re-send">re-send</a></p></div><div id="pf-dialog-pdf"><div id="pdf-frame-container"></div></div></div><div class="pf-a flex-grow-1 js-ads-dialog d-flex flex-row justify-content-around align-items-stretch w-100 py-4 print-no"><div class="a-box">

<div id="div-gpt-ad-1562914060901-0">
  <script>
    googletag.cmd.push(function () {
      deployads.push(function() {
        deployads.gpt.display('div-gpt-ad-1562914060901-0'); });
    });
  </script>
</div>
</div><div class="a-box-right">
<!-- /21822432912/pfcom-box2 -->

<!-- /21822432912/pfcom-box2 -->
<div id="div-gpt-ad-1574251754981-0">
  <script>
    googletag.cmd.push(function() {
      deployads.push(function() {
        deployads.gpt.display('div-gpt-ad-1574251754981-0'); });
      });
  </script>
</div>
</div></div><script>Settings.adType = 'publisher_group_2';
Settings.loadAdScripts = true;</script></div></div><div class="browser pf-content-wrapper px-2 px-sm-4 px-md-5 pt-2 pt-sm-3" id="pf-body"><div class="pf-12 print-only" id="printfriendly"><div><div class="clearfix js-print-area content-unmask direction-ltr" id="pf-print-area"><div class="print-header"></div><h1 id="pf-title" class="non-delete">Distributed systems</h1><div id="pf-src" class="non-delete"><a id="pf-src-url" href="http://book.mixu.net/distsys/ebook.html" class="non-delete"><img id="pf-src-icon" src="./Distributed systems_files/68747470733a2f2f73322e676f6f676c6575736572636f6e74656e742e636f6d2f73322f66617669636f6e733f646f6d61696e3d626f6f6b2e6d6978752e6e6574" class="non-delete"><strong class="non-delete">book.mixu.net</strong>/distsys/ebook.html</a></div><div style="clear:both"></div><span id="pf-author"></span><span id="pf-date"></span><div class="js-pf-content pf-12" id="pf-content"><div id="content" class="" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="94732.5" orig-style="null">
<h2 id="introduction" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Introduction</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">I wanted a text that would bring together the ideas behind many of the more recent distributed systems - systems such as Amazon's Dynamo, Google's BigTable and MapReduce, Apache's Hadoop and so on.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">In this text I've tried to provide a more accessible introduction to distributed systems. To me, that means two things: introducing the key concepts that you will need in order to </span><a href="https://www.google.com/search?q=super+cool+ski+instructor" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="138.984375" data-pf_rect_height="19" orig-style="null"><span class="text-node">have a good time</span></a><span class="text-node"> reading more serious texts, and providing a narrative that covers things in enough detail that you get a gist of what's going on without getting stuck on details. It's 2013, you've got the Internet, and you can selectively read more about the topics you find most interesting.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">In my view, much of distributed programming is about dealing with the implications of two consequences of distribution:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null" class=""><span class="text-node">that information travels at the speed of light</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null" class=""><span class="text-node">that independent things fail independently*</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">In other words, that the core of distributed programming is dealing with distance (duh!) and having more than one thing (duh!). These constraints define a space of possible system designs, and my hope is that after reading this you'll have a better sense of how distance, time and consistency models interact.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">This text is focused on distributed programming and systems concepts you'll need to understand commercial systems in the data center. It would be madness to attempt to cover everything. You'll learn many key protocols and algorithms (covering, for example, many of the most cited papers in the discipline), including some new exciting ways to look at eventual consistency that haven't still made it into college textbooks - such as CRDTs and the CALM theorem.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">I hope you like it! If you want to say thanks, follow me on </span><a href="https://github.com/mixu/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="55.140625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Github</span></a><span class="text-node"> (or </span><a href="http://twitter.com/mikitotakada" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="58.84375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Twitter</span></a><span class="text-node">). And if you spot an error, </span><a href="https://github.com/mixu/distsysbook/issues" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="228.078125" data-pf_rect_height="19" orig-style="null"><span class="text-node">file a pull request on Github</span></a><span class="text-node">.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h1 id="1-basics" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">1. Basics</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><a href="http://book.mixu.net/distsys/ebook.html#intro" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="136.4375" data-pf_rect_height="19" orig-style="null"><span class="text-node">The first chapter</span></a><span class="text-node"> covers distributed systems at a high level by introducing a number of important terms and concepts. It covers high level goals, such as scalability, availability, performance, latency and fault tolerance; how those are hard to achieve, and how abstractions and models as well as partitioning and replication come into play.</span></p>
<h1 id="2-up-and-down-the-level-of-abstraction" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null"><span class="text-node">2. Up and down the level of abstraction</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><a href="http://book.mixu.net/distsys/ebook.html#abstractions" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="159.921875" data-pf_rect_height="19" orig-style="null"><span class="text-node">The second chapter</span></a><span class="text-node"> dives deeper into abstractions and impossibility results. It starts with a Nietzsche quote, and then introduces system models and the many assumptions that are made in a typical system model. It then discusses the CAP theorem and summarizes the FLP impossibility result. It then turns to the implications of the CAP theorem, one of which is that one ought to explore other consistency models. A number of consistency models are then discussed.</span></p>
<h1 id="3-time-and-order" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">3. Time and order</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">A big part of understanding distributed systems is about understanding time and order.  To the extent that we fail to understand and model time, our systems will fail. </span><a href="http://book.mixu.net/distsys/ebook.html#time" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="383.71875" data-pf_rect_height="38" orig-style="null"><span class="text-node">The third chapter</span></a><span class="text-node"> discusses time and order, and clocks as well as the various uses of time, order and clocks (such as vector clocks and failure detectors).</span></p>
<h1 id="4-replication-preventing-divergence" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null"><span class="text-node">4. Replication: preventing divergence</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The </span><a href="http://book.mixu.net/distsys/ebook.html#replication" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="117.828125" data-pf_rect_height="19" orig-style="null"><span class="text-node">fourth chapter</span></a><span class="text-node"> introduces the replication problem, and the two basic ways in which it can be performed. It turns out that most of the relevant characteristics can be discussed with just this simple characterization. Then, replication methods for maintaining single-copy consistency are discussed from the least fault tolerant (2PC) to Paxos.</span></p>
<h1 id="5-replication-accepting-divergence" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null"><span class="text-node">5. Replication: accepting divergence</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">The </span><a href="http://book.mixu.net/distsys/ebook.html#eventual" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="101.296875" data-pf_rect_height="19" orig-style="null"><span class="text-node">fifth chapter</span></a><span class="text-node"> discussed replication with weak consistency guarantees. It introduces a basic reconciliation scenario, where partitioned replicas attempt to reach agreement. It then discusses Amazon's Dynamo as an example of a system design with weak consistency guarantees. Finally, two perspectives on disorderly programming are discussed: CRDTs and the CALM theorem.</span></p>
<h1 id="appendix" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">Appendix</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><a href="http://book.mixu.net/distsys/ebook.html#appendix" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="109.640625" data-pf_rect_height="19" orig-style="null"><span class="text-node">The appendix</span></a><span class="text-node"> covers recommendations for further reading.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<p class="footnote added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">*: This is a </span><a href="http://en.wikipedia.org/wiki/Statistical_independence" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="19.6875" data-pf_rect_height="19" orig-style="null"><span class="text-node">lie</span></a><span class="text-node">. </span><a href="http://blog.empathybox.com/post/19574936361/getting-real-about-distributed-system-reliability" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="301.484375" data-pf_rect_height="38" orig-style="null"><span class="text-node">This post by Jay Kreps elaborates</span></a><span class="text-node">.
</span></p>

<h1 id="-chapter_number-distributed-systems-at-a-high-level" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null"><span class="text-node">1. Distributed systems at a high level</span></h1>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">There are two basic tasks that any computer system needs to accomplish:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">storage and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">computation</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers - usually, because the problem no longer fits on a single computer.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Nothing really demands that you use distributed systems. Given infinite money and infinite R&amp;D time, we wouldn't need distributed systems. All computation and storage could be done on a magic box - a single, incredibly fast and incredibly reliable system </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="356.171875" data-pf_rect_height="38" orig-style="null"><span class="text-node">that you pay someone else to design for you</span></em><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">However, few people have infinite resources. Hence, they have to find the right place on some real-world cost-benefit curve. At a small scale, upgrading hardware is a viable strategy. However, as problem sizes increase you will reach a point where either the hardware upgrade that allows you to solve the problem on a single node does not exist, or becomes cost-prohibitive. At that point, I welcome you to the world of distributed systems.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">It is a current reality that the best value is in mid-range, commodity hardware - as long as the maintenance costs can be kept down through fault-tolerant software.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Computations primarily benefit from high-end hardware to the extent to which they can replace slow network accesses with internal memory accesses. The performance advantage of high-end hardware is limited in tasks that require large amounts of communication between nodes.</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f626172726f736f5f686f6c7a6c652e706e67" alt="cost-efficiency" pf-orig-src="http://book.mixu.net/distsys/images/barroso_holzle.png" pf-restore-src="https://pdf.printfriendly.com/camo/704a6355c9f66dcc231b331cdae402d1af40a017/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f626172726f736f5f686f6c7a6c652e706e67" pf-data-width="621" pf-data-height="359" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="121" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">As the figure above from </span><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="367.328125" data-pf_rect_height="38" orig-style="null"><span class="text-node">Barroso, Clidaras &amp; Hölzle</span></a><span class="text-node"> shows, the performance gap between high-end and commodity hardware decreases with cluster size assuming a uniform memory access pattern across all nodes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">Ideally, adding a new machine would increase the performance and capacity of the system linearly. But of course this is not possible, because there is some overhead that arises due to having separate computers. Data needs to be copied around, computation tasks have to be coordinated and so on. This is why it's worthwhile to study distributed algorithms - they provide efficient solutions to specific problems, as well as guidance about what is possible, what the minimum cost of a correct implementation is, and what is impossible.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">The focus of this text is on distributed programming and systems in a mundane, but commercially relevant setting: the data center. For example, I will not discuss specialized problems that arise from having an exotic network configuration, or that arise in a shared-memory setting. Additionally, the focus is on exploring the system design space rather than on optimizing any specific design - the latter is a topic for a much more specialized text.</span></p>
<h2 id="what-we-want-to-achieve-scalability-and-other-good-things" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="87" orig-style="null"><span class="text-node">What we want to achieve: Scalability and other good things</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">The way I see it, everything starts with the need to deal with size.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Most things are trivial at a small scale - and the same problem becomes much harder once you surpass a certain size, volume or other physically constrained thing. It's easy to lift a piece of chocolate, it's hard to lift a mountain. It's easy to count how many people are in a room, and hard to count how many people are in a country.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">So everything starts with size - scalability. Informally speaking, in a scalable system as we move from small to large, things should not get incrementally worse. Here's another definition:</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><a href="http://en.wikipedia.org/wiki/Scalability" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="85.09375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Scalability</span></a></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">What is it that is growing? Well, you can measure growth in almost any terms (number of people, electricity usage etc.). But there are three particularly interesting things to look at:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="266" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Size scalability: adding more nodes should make the system linearly faster; growing the dataset should not increase latency</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null"><span class="text-node">Geographic scalability: it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Administrative scalability: adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Of course, in a real system growth occurs on multiple different axes simultaneously; each metric captures just some aspect of growth.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">A scalable system is one that continues to meet the needs of its users as scale increases. There are two particularly relevant aspects - performance and availability - which can be measured in various ways.</span></p>
<h3 id="performance-and-latency-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Performance (and latency)</span></h3>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><a href="http://en.wikipedia.org/wiki/Computer_performance" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="103.765625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Performance</span></a></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Depending on the context, this may involve achieving one or more of the following:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Short response time/low latency for a given piece of work</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">High throughput (rate of processing work)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Low utilization of computing resource(s)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">There are tradeoffs involved in optimizing for any of these outcomes. For example, a system may achieve a higher throughput by processing larger batches of work thereby reducing operation overhead. The tradeoff would be longer response times for individual pieces of work due to batching.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">I find that low latency - achieving a short response time - is the most interesting aspect of performance, because it has a strong connection with physical (rather than financial) limitations. It is harder to address latency using financial resources than the other aspects of performance.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">There are a lot of really specific definitions for latency, but I really like the idea that the etymology of the word evokes:</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Latency</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">The state of being latent; delay, a period between the initiation of something and the occurrence.</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">And what does it mean to be "latent"?</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Latent</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">From Latin latens, latentis, present participle of lateo ("lie hidden"). Existing or present but concealed or inactive.</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">This definition is pretty cool, because it highlights how latency is really the time between when something happened and the time it has an impact or becomes visible.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">For example, imagine that you are infected with an airborne virus that turns people into zombies. The latent period is the time between when you became infected, and when you turn into a zombie. That's latency: the time during which something that has already happened is concealed from view.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Let's assume for a moment that our distributed system does just one high-level task: given a query, it takes all of the data in the system and calculates a single result. In other words, think of a distributed system as a data store with the ability to run a single deterministic computation (function) over its current content:</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="296.875" data-pf_rect_height="15" orig-style="null">result = query(all data in the system)</code></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Then, what matters for latency is not the amount of old data, but rather the speed at which new data "takes effect" in the system. For example, latency could be measured in terms of how long it takes for a write to become visible to readers.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The other key point based on this definition is that if nothing happens, there is no "latent period". A system in which data doesn't change doesn't (or shouldn't) have a latency problem.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">In a distributed system, there is a minimum latency that cannot be overcome: the speed of light limits how fast information can travel, and hardware components have a minimum latency cost incurred per operation (think RAM and hard drives but also CPUs).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">How much that minimum latency impacts your queries depends on the nature of those queries and the physical distance the information needs to travel.</span></p>
<h3 id="availability-and-fault-tolerance-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Availability (and fault tolerance)</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">The second aspect of a scalable system is availability.</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><a href="http://en.wikipedia.org/wiki/High_availability" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="90.21875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Availability</span></a></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable. </span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Distributed systems allow us to achieve desirable characteristics that would be hard to accomplish on a single system. For example, a single machine cannot tolerate any failures since it either fails or doesn't.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Distributed systems can take a bunch of unreliable components, and build a reliable system on top of them.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Systems that have no redundancy can only be as available as their underlying components. Systems built with redundancy can be tolerant of partial failures and thus be more available. It is worth noting that "redundant" can mean different things depending on what you look at - components, servers, datacenters and so on.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Formulaically, availability is: </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="345.5625" data-pf_rect_height="34" orig-style="null">Availability = uptime / (uptime + downtime)</code><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Availability from a technical perspective is mostly about being fault tolerant. Because the probability of a failure occurring increases with the number of components, the system should be able to compensate so as to not become less reliable as the number of components increases.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">For example:</span></p>
<table data-pf_style_display="table" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="296" orig-style="null">
<tbody data-pf_style_display="table-row-group" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="296" orig-style="null">
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">Availability %</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">How much downtime is allowed per year?</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">90% ("one nine")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">More than a month</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">99% ("two nines")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">Less than 4 days</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">99.9% ("three nines")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">Less than 9 hours</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">99.99% ("four nines")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">Less than an hour</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">99.999% ("five nines")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">~ 5 minutes</span></td>
</tr>
<tr data-pf_style_display="table-row" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="40" orig-style="null">
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="133" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">99.9999% ("six nines")</span></td>
  <td data-pf_style_display="table-cell" data-pf_style_visibility="visible" data-pf_rect_width="245" data-pf_rect_height="40" class="added-to-list1" orig-style="null"><span class="text-node">~ 31 seconds</span></td>
</tr>
</tbody>
</table>


<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Availability is in some sense a much wider concept than uptime, since the availability of a service can also be affected by, say, a network outage or the company owning the service going out of business (which would be a factor which is not really relevant to fault tolerance but would still influence the availability of the system). But without knowing every single specific aspect of the system, the best we can do is design for fault tolerance.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">What does it mean to be fault tolerant?</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Fault tolerance</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">ability of a system to behave in a well-defined manner once faults occur</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Fault tolerance boils down to this: define what faults you expect and then design a system or an algorithm that is tolerant of them. You can't tolerate faults you haven't considered.</span></p>
<h2 id="what-prevents-us-from-achieving-good-things-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">What prevents us from achieving good things?</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Distributed systems are constrained by two physical factors:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">the number of nodes (which increases with the required storage and computation capacity)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">the distance between nodes (information travels, at best, at the speed of light)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">Working within those constraints:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">an increase in the number of independent nodes increases the probability of failure in a system (reducing availability and increasing administrative costs)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">an increase in the number of independent nodes may increase the need for communication between nodes (reducing performance as scale increases)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">an increase in geographic distance increases the minimum latency for communication between distant nodes (reducing performance for certain operations)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Beyond these tendencies - which are a result of the physical constraints - is the world of system design options.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">Both performance and availability are defined by the external guarantees the system makes. On a high level, you can think of the guarantees as the SLA (service level agreement) for the system: if I write data, how quickly can I access it elsewhere? After the data is written, what guarantees do I have of durability? If I ask the system to run a computation, how quickly will it return results? When components fail, or are taken out of operation, what impact will this have on the system?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">There is another criterion, which is not explicitly mentioned but implied: intelligibility. How understandable are the guarantees that are made? Of course, there are no simple metrics for what is intelligible.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">I was kind of tempted to put "intelligibility" under physical limitations. After all, it is a hardware limitation in people that we have a hard time understanding anything that involves </span><a href="http://en.wikipedia.org/wiki/Working_memory#Capacity" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="346.3125" data-pf_rect_height="38" orig-style="null"><span class="text-node">more moving things than we have fingers</span></a><span class="text-node">. That's the difference between an error and an anomaly - an error is incorrect behavior, while an anomaly is unexpected behavior. If you were smarter, you'd expect the anomalies to occur.</span></p>
<h2 id="abstractions-and-models" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Abstractions and models</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">This is where abstractions and models come into play. Abstractions make things more manageable by removing real-world aspects that are not relevant to solving a problem. Models describe the key properties of a distributed system in a precise manner. I'll discuss many kinds of models in the next chapter, such as:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">System model (asynchronous / synchronous)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Failure model (crash-fail, partitions, Byzantine)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Consistency model (strong, eventual)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">A good abstraction makes working with a system easier to understand, while capturing the factors that are relevant for a particular purpose.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">There is a tension between the reality that there are many nodes and with our desire for systems that "work like a single system". Often, the most familiar model (for example, implementing a shared memory abstraction on a distributed system) is too expensive.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">A system that makes weaker guarantees has more freedom of action, and hence potentially greater performance - but it is also potentially hard to reason about. People are better at reasoning about systems that work like a single system, rather than a collection of nodes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">One can often gain performance by exposing more details about the internals of the system. For example, in </span><a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="142.828125" data-pf_rect_height="19" orig-style="null"><span class="text-node">columnar storage</span></a><span class="text-node">, the user can (to some extent) reason about the locality of the key-value pairs within the system and hence make decisions that influence the performance of typical queries. Systems which hide these kinds of details are easier to understand (since they act more like single unit, with fewer details to think about), while systems that expose more real-world details may be more performant (because they correspond more closely to reality).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Several types of failures make writing distributed systems that act like a single system difficult. Network latency and network partitions (e.g. total network failure between some nodes) mean that a system needs to sometimes make hard choices about whether it is better to stay available but lose some crucial guarantees that cannot be enforced, or to play it safe and refuse clients when these types of failures occur.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">The CAP theorem - which I will discuss in the next chapter - captures some of these tensions. In the end, the ideal system meets both programmer needs (clean semantics) and business needs (availability/consistency/latency).</span></p>
<h2 id="design-techniques-partition-and-replicate" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Design techniques: partition and replicate</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication).</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null" class="">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Divide and conquer - I mean, partition and replicate.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations.</span></p>
<p class="img-container added-to-list1 pf-delete" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f706172742d7265706c2e706e67" alt="Partition and replicate" pf-orig-src="http://book.mixu.net/distsys/images/part-repl.png" pf-restore-src="https://pdf.printfriendly.com/camo/7dd954cc35a15fd4142dc45561fade519f247608/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f706172742d7265706c2e706e67" pf-data-width="426" pf-data-height="456" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="187" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives.</span></p>
<h3 id="partitioning" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Partitioning</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partition</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Partitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificed</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Partitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That's why the focus is on replication in most texts, including this one.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Partitioning is mostly about defining your partitions based on what you think the primary access pattern will be, and dealing with the limitations that come from having independent partitions (e.g. inefficient access across partitions, different rate of growth etc.).</span></p>
<h3 id="replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Replication</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">Let me inaccurately quote </span><a href="http://en.wikipedia.org/wiki/Homer_vs._the_Eighteenth_Amendment" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="145.84375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Homer J. Simpson</span></a><span class="text-node">:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">To replication! The cause of, and solution to all of life's problems.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Replication - copying or reproducing something - is the primary way in which we can fight latency.</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the data</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Replication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificed</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Replication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h2 id="further-reading" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Further reading</span></h2>


<h1 id="-chapter_number-up-and-down-the-level-of-abstraction" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null"><span class="text-node">2. Up and down the level of abstraction</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">In this chapter, we'll travel up and down the level of abstraction, look at some impossibility results (CAP and FLP), and then travel back down for the sake of performance.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">If you've done any programming, the idea of levels of abstraction is probably familiar to you. You'll always work at some level of abstraction, interface with a lower level layer through some API, and probably provide some higher-level API or user interface to your users. The seven-layer </span><a href="http://en.wikipedia.org/wiki/OSI_model" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="375.796875" data-pf_rect_height="38" orig-style="null"><span class="text-node">OSI model of computer networking</span></a><span class="text-node"> is a good example of this.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Distributed programming is, I'd assert, in large part dealing with consequences of distribution (duh!). That is, there is a tension between the reality that there are many nodes and with our desire for systems that "work like a single system". That means finding a good abstraction that balances what is possible with what is understandable and performant.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">What do we mean when say X is more abstract than Y? First, that X does not introduce anything new or fundamentally different from Y. In fact, X may remove some aspects of Y or present them in a way that makes them more manageable.
Second, that X is in some sense easier to grasp than Y, assuming that the things that X removed from Y are not important to the matter at hand.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">As </span><a href="http://oregonstate.edu/instruct/phl201/modules/Philosophers/Nietzsche/Truth_and_Lie_in_an_Extra-Moral_Sense.htm" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="80.34375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Nietzsche</span></a><span class="text-node"> wrote:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="323" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="323" class="added-to-list1" orig-style="null"><span class="text-node">Every concept originates through our equating what is unequal. No leaf ever wholly equals another, and the concept "leaf" is formed through an arbitrary abstraction from these individual differences, through forgetting the distinctions; and now it gives rise to the idea that in nature there might be something besides the leaves which would be "leaf" - some kind of original form after which all leaves have been woven, marked, copied, colored, curled, and painted, but by unskilled hands, so that no copy turned out to be a correct, reliable, and faithful image of the original form.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Abstractions, fundamentally, are fake. Every situation is unique, as is every node. But abstractions make the world manageable: simpler problem statements - free of reality - are much more analytically tractable and provided that we did not ignore anything essential, the solutions are widely applicable.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Indeed, if the things that we kept around are essential, then the results we can derive will be widely applicable. This is why impossibility results are so important: they take the simplest possible formulation of a problem, and demonstrate that it is impossible to solve within some set of constraints or assumptions.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">All abstractions ignore something in favor of equating things that are in reality unique. The trick is to get rid of everything that is not essential. How do you know what is essential? Well, you probably won't know a priori.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Every time we exclude some aspect of a system from our specification of the system, we risk introducing a source of error and/or a performance issue. That's why sometimes we need to go in the other direction, and selectively introduce some aspects of real hardware and the real-world problem back. It may be sufficient to reintroduce some specific hardware characteristics (e.g. physical sequentiality) or other physical characteristics to get a system that performs well enough.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">With this in mind, what is the least amount of reality we can keep around while still working with something that is still recognizable as a distributed system? A system model is a specification of the characteristics we consider important; having specified one, we can then take a look at some impossibility results and challenges.</span></p>
<h2 id="a-system-model" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">A system model</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">A key property of distributed systems is distribution. More specifically, programs in a distributed system:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">run concurrently on independent nodes ...</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">are connected by a network that may introduce nondeterminism and message loss ...</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">and have no shared memory or shared clock.</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">There are many implications:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="304" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">each node executes a program concurrently</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">knowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of date</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">nodes can fail and recover from failure independently</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">messages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">A system model enumerates the many assumptions associated with a particular system design.</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">System model</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">a set of assumptions about the environment and facilities on which a distributed system is implemented</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">System models vary in their assumptions about the environment and facilities. These assumptions include:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">what capabilities the nodes have and how they may fail</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">how communication links operate and how they may fail and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">properties of the overall system, such as assumptions about time and order</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">A robust system model is one that makes the weakest assumptions: any algorithm written for such a system is very tolerant of different environments, since it makes very few and very weak assumptions.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">On the other hand, we can create a system model that is easy to reason about by making strong assumptions. For example, assuming that nodes do not fail means that our algorithm does not need to handle node failures. However, such a system model is unrealistic and hence hard to apply into practice.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Let's look at the properties of nodes, links and time and order in more detail.</span></p>
<h3 id="nodes-in-our-system-model" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Nodes in our system model</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Nodes serve as hosts for computation and storage. They have:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">the ability to execute a program</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">a clock (which may or may not be assumed to be accurate)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Nodes execute deterministic algorithms: the local computation, the local state after the computation, and the messages sent are determined uniquely by the message received and local state when the message was received.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">There are many possible failure models which describe the ways in which nodes can fail. In practice, most systems assume a crash-recovery failure model: that is, nodes can only fail by crashing, and can (possibly) recover after crashing at some later point.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Another alternative is to assume that nodes can fail by misbehaving in any arbitrary way. This is known as </span><a href="http://en.wikipedia.org/wiki/Byzantine_fault_tolerance" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="204.28125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Byzantine fault tolerance</span></a><span class="text-node">. Byzantine faults are rarely handled in real world commercial systems, because algorithms resilient to arbitrary faults are more expensive to run and more complex to implement. I will not discuss them here.</span></p>
<h3 id="communication-links-in-our-system-model" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Communication links in our system model</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Communication links connect individual nodes to each other, and allow messages to be sent in either direction. Many books that discuss distributed algorithms assume that there are individual links between each pair of nodes, that the links provide FIFO (first in, first out) order for messages, that they can only deliver messages that were sent, and that sent messages can be lost.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Some algorithms assume that the network is reliable: that messages are never lost and never delayed indefinitely. This may be a reasonable assumption for some real-world settings, but in general it is preferable to consider the network to be unreliable and subject to message loss and delays.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">A network partition occurs when the network fails while the nodes themselves remain operational. When this occurs, messages may be lost or delayed until the network partition is repaired. Partitioned nodes may be accessible by some clients, and so must be treated differently from crashed nodes. The diagram below illustrates a node failure vs. a network partition:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d322e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/system-of-2.png" pf-restore-src="https://pdf.printfriendly.com/camo/82720236d76a90158480f396c9117503fad9973c/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d322e706e67" pf-data-width="739" pf-data-height="216" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="92" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">It is rare to make further assumptions about communication links. We could assume that links only work in one direction, or we could introduce different communication costs (e.g. latency due to physical distance) for different links. However, these are rarely concerns in commercial environments except for long-distance links (WAN latency) and so I will not discuss them here; a more detailed model of costs and topology allows for better optimization at the cost of complexity.</span></p>
<h3 id="timing-ordering-assumptions" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Timing / ordering assumptions</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">One of the consequences of physical distribution is that each node experiences the world in a unique manner. This is inescapable, because information can only travel at the speed of light. If nodes are at different distances from each other, then any messages sent from one node to the others will arrive at a different time and potentially in a different order at the other nodes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Timing assumptions are a convenient shorthand for capturing assumptions about the extent to which we take this reality into account. The two main alternatives are:</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Synchronous system model</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock</span></dd>
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Asynchronous system model</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">The synchronous system model imposes many constraints on time and order. It essentially assumes that the nodes have the same experience: that messages that are sent are always received within a particular maximum transmission delay, and that processes execute in lock-step. This is convenient, because it allows you as the system designer to make assumptions about time and order, while the asynchronous system model doesn't.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Asynchronicity is a non-assumption: it just assumes that you can't rely on timing (or a "time sensor").</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">It is easier to solve problems in the synchronous system model, because assumptions about execution speeds, maximum message transmission delays and clock accuracy all help in solving problems since you can make inferences based on those assumptions and rule out inconvenient failure scenarios by assuming they never occur.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">Of course, assuming the synchronous system model is not particularly realistic. Real-world networks are subject to failures and there are no hard bounds on message delay. Real world systems are at best partially synchronous: they may occasionally work correctly and provide some upper bounds, but there will be times where messages are delayed indefinitely and clocks are out of sync. I won't really discuss algorithms for synchronous systems here, but you will probably run into them in many other introductory books because they are analytically easier (but unrealistic).</span></p>
<h3 id="the-consensus-problem" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">The consensus problem</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">During the rest of this text, we'll vary the parameters of the system model. Next, we'll look at how varying two system properties:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">whether or not network partitions are included in the failure model, and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">synchronous vs. asynchronous timing assumptions</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">influence the system design choices by discussing two impossibility results (FLP and CAP).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Of course, in order to have a discussion, we also need to introduce a problem to solve. The problem I'm going to discuss is the </span><a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="372.703125" data-pf_rect_height="38" orig-style="null"><span class="text-node">consensus problem</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Several computers (or nodes) achieve consensus if they all agree on some value. More formally:</span></p>
<ol class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Agreement: Every correct process must agree on the same value.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Termination: All processes eventually reach a decision.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">Validity: If all correct processes propose the same value V, then all correct processes decide V.</span></li>
</ol>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">The consensus problem is at the core of many commercial distributed systems. After all, we want the reliability and performance of a distributed system without having to deal with the consequences of distribution (e.g. disagreements / divergence between nodes), and solving the consensus problem makes it possible to solve several related, more advanced problems such as atomic broadcast and atomic commit.</span></p>
<h3 id="two-impossibility-results" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Two impossibility results</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">The first impossibility result, known as the FLP impossibility result, is an impossibility result that is particularly relevant to people who design distributed algorithms. The second - the CAP theorem - is a related result that is more relevant to practitioners; people who need to choose between different system designs but who are not directly concerned with the design of algorithms.</span></p>
<h2 id="the-flp-impossibility-result" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">The FLP impossibility result</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="266" class="added-to-list1" orig-style="null"><span class="text-node">I will only briefly summarize the </span><a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29#Solvability_results_for_some_agreement_problems" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="300.0625" data-pf_rect_height="38" orig-style="null"><span class="text-node">FLP impossibility result</span></a><span class="text-node">, though it is considered to be </span><a href="http://en.wikipedia.org/wiki/Dijkstra_Prize" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="127.4375" data-pf_rect_height="19" orig-style="null"><span class="text-node">more important</span></a><span class="text-node"> in academic circles. The FLP impossibility result (named after the authors, Fischer, Lynch and Patterson) examines the consensus problem under the asynchronous system model (technically, the agreement problem, which is a very weak form of the consensus problem). It is assumed that nodes can only fail by crashing; that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: e.g. there are no bounds on message delay.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Under these assumptions, the FLP result states that "there does not exist a (deterministic) algorithm for the consensus problem in an asynchronous system subject to failures, even if messages can never be lost, at most one process may fail, and it can only fail by crashing (stopping executing)".</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">This result means that there is no way to solve the consensus problem under a very minimal system model in a way that cannot be delayed forever.  The argument is that if such an algorithm existed, then one could devise an execution of that algorithm in which it would remain undecided ("bivalent") for an arbitrary amount of time by delaying message delivery - which is allowed in the asynchronous system model. Thus, such an algorithm cannot exist.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">This impossibility result is important because it highlights that assuming the asynchronous system model leads to a tradeoff: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">This insight is particularly relevant to people who design algorithms, because it imposes a hard constraint on the problems that we know are solvable in the asynchronous system model. The CAP theorem is a related theorem that is more relevant to practitioners: it makes slightly different assumptions (network failures rather than node failures), and has more clear implications for practitioners choosing between system designs.</span></p>
<h2 id="the-cap-theorem" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">The CAP theorem</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The CAP theorem was initially a conjecture made by computer scientist Eric Brewer. It's a popular and fairly useful way to think about tradeoffs in the guarantees that a system design makes. It even has a </span><a href="https://www.google.com/search?q=Brewer%27s+conjecture+and+the+feasibility+of+consistent%2C+available%2C+partition-tolerant+web+services" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="101.125" data-pf_rect_height="19" orig-style="null"><span class="text-node">formal proof</span></a><span class="text-node"> by </span><a href="http://www.comp.nus.edu.sg/~gilbert/biblio.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="56.765625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Gilbert</span></a><span class="text-node"> and </span><a href="http://en.wikipedia.org/wiki/Nancy_Lynch" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="48.921875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Lynch</span></a><span class="text-node"> and no, </span><a href="http://nathanmarz.com/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="107.125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Nathan Marz</span></a><span class="text-node"> didn't debunk it, in spite of what </span><a href="http://news.ycombinator.com/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="312.703125" data-pf_rect_height="38" orig-style="null"><span class="text-node">a particular discussion site</span></a><span class="text-node"> thinks.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">The theorem states that of these three properties:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Consistency: all nodes see the same data at the same time.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Availability: node failures do not prevent survivors from continuing to operate.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">Partition tolerance: the system continues to operate despite message loss due to network and/or node failure</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">only two can be satisfied simultaneously. We can even draw this as a pretty diagram, picking two properties out of three gives us three types of systems that correspond to different intersections:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f4341502e706e67" alt="CAP theorem" pf-orig-src="http://book.mixu.net/distsys/images/CAP.png" pf-restore-src="https://pdf.printfriendly.com/camo/8b5241034bdfeff19b3bdf02d18e22f5bf058675/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f4341502e706e67" pf-data-width="405" pf-data-height="377" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="112" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Note that the theorem states that the middle piece (having all three properties) is not achievable. Then we get three different system types:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo.</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">The CA and CP system designs both offer the same consistency model: strong consistency. The only difference is that a CA system cannot tolerate any node failures; a CP system can tolerate up to </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">f</code><span class="text-node"> faults given </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="31.25" data-pf_rect_height="15" orig-style="null">2f+1</code><span class="text-node"> nodes in a non-Byzantine failure model (in other words, it can tolerate the failure of a minority </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">f</code><span class="text-node"> of the nodes as long as majority </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">f+1</code><span class="text-node"> stays up). The reason is simple:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="361" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="171" orig-style="null"><span class="text-node">A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="190" orig-style="null"><span class="text-node">A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency.</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">I'll discuss this in more detail in the chapter on replication when I discuss Paxos. The important thing is that CP systems incorporate network partitions into their failure model and distinguish between a majority partition and a minority partition using an algorithm like Paxos, Raft or viewstamped replication. CA systems are not partition-aware, and are historically more common: they often use the two-phase commit algorithm and are common in traditional distributed relational databases.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Assuming that a partition occurs, the theorem reduces to a binary choice between availability and consistency.</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f4341505f63686f6963652e706e67" alt="Based on http://blog.mikiobraun.de/2013/03/misconceptions-about-cap-theorem.html" pf-orig-src="http://book.mixu.net/distsys/images/CAP_choice.png" pf-restore-src="https://pdf.printfriendly.com/camo/fca2cb0bc7b5ccc2880334caea5cb4cd1c971d9c/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f4341505f63686f6963652e706e67" pf-data-width="399" pf-data-height="247" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="695" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">I think there are four conclusions that should be drawn from the CAP theorem:</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">First, that </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="382.390625" data-pf_rect_height="57" orig-style="null"><span class="text-node">many system designs used in early distributed relational database systems did not take into account partition tolerance</span></em><span class="text-node"> (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Second, that </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="379.859375" data-pf_rect_height="57" orig-style="null"><span class="text-node">there is a tension between strong consistency and high availability during network partitions</span></em><span class="text-node">. The CAP theorem is an illustration of the tradeoffs that occur between strong guarantees and distributed computation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">In some sense, it is quite crazy to promise that a distributed system consisting of independent nodes connected by an unpredictable network "behaves in a way that is indistinguishable from a non-distributed system".</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6e6577735f3132302e6a7067" alt="From the Simpsons episode Trash of the Titans" pf-orig-src="http://book.mixu.net/distsys/images/news_120.jpg" pf-restore-src="https://pdf.printfriendly.com/camo/0a4e854f50615bd28ad38978fe4b191a23e39d10/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6e6577735f3132302e6a7067" pf-data-width="290" pf-data-height="211" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="388" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full mediumImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Strong consistency guarantees require us to give up availability during a partition. This is because one cannot prevent divergence between two replicas that cannot communicate with each other while continuing to accept writes on both sides of the partition.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">How can we work around this? By strengthening the assumptions (assume no partitions) or by weakening the guarantees. Consistency can be traded off against availability (and the related capabilities of offline accessibility and low latency). If "consistency" is defined as something less than "all nodes see the same data at the same time" then we can have both availability and some (weaker) consistency guarantee.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Third, that </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="364.28125" data-pf_rect_height="57" orig-style="null"><span class="text-node">there is a tension between strong consistency and performance in normal operation</span></em><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Strong consistency / single-copy consistency requires that nodes communicate and agree on every operation. This results in high latency during normal operation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">If you can live with a consistency model other than the classic one, a consistency model that allows replicas to lag or to diverge, then you can reduce latency during normal operation and maintain availability in the presence of partitions.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">When fewer messages and fewer nodes are involved, an operation can complete faster. But the only way to accomplish that is to relax the guarantees: let some of the nodes be contacted less frequently, which means that nodes can contain old data.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">This also makes it possible for anomalies to occur. You are no longer guaranteed to get the most recent value. Depending on what kinds of guarantees are made, you might read a value that is older than expected, or even lose some updates.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Fourth - and somewhat indirectly - that </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="377.46875" data-pf_rect_height="95" orig-style="null"><span class="text-node">if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes</span></em><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">For example, even if user data is georeplicated to multiple datacenters, and the link between those two datacenters is temporarily out of order, in many cases we'll still want to allow the user to use the website / service. This means reconciling two divergent sets of data later on, which is both a technical challenge and a business risk. But often both the technical challenge and the business risk are manageable, and so it is preferable to provide high availability.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Consistency and availability are not really binary choices, unless you limit yourself to strong consistency. But strong consistency is just one consistency model: the one where you, by necessity, need to give up availability in order to prevent more than a single copy of the data from being active. As </span><a href="http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="373.4375" data-pf_rect_height="38" orig-style="null"><span class="text-node">Brewer himself points out</span></a><span class="text-node">, the "2 out of 3" interpretation is misleading.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">If you take away just one idea from this discussion, let it be this: "consistency" is not a singular, unambiguous property. Remember:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null">
  <p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" class="added-to-list1" orig-style="null">
   <a href="http://en.wikipedia.org/wiki/ACID" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="42.921875" data-pf_rect_height="19" orig-style="null"><span class="text-node">ACID</span></a><span class="text-node"> consistency != </span><br data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="0" data-pf_rect_height="0" orig-style="null">
   <a href="http://en.wikipedia.org/wiki/CAP_theorem" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="34.546875" data-pf_rect_height="19" orig-style="null"><span class="text-node">CAP</span></a><span class="text-node"> consistency != </span><br data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="0" data-pf_rect_height="0" orig-style="null">
   <a href="http://en.wikipedia.org/wiki/Oatmeal" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="68.34375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Oatmeal</span></a><span class="text-node"> consistency
  </span></p>
</blockquote>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Instead, a consistency model is a guarantee - any guarantee - that a data store gives to programs that use it.</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Consistency model</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">The "C" in CAP is "strong consistency", but "consistency" is not a synonym for "strong consistency".</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Let's take a look at some alternative consistency models.</span></p>
<h2 id="strong-consistency-vs-other-consistency-models" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Strong consistency vs. other consistency models</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Consistency models can be categorized into two types: strong and weak consistency models:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null">
<span class="text-node">Strong consistency models (capable of maintaining a single copy)</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="19" orig-style="null"><span class="text-node">Linearizable consistency</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="19" orig-style="null"><span class="text-node">Sequential consistency</span></li>
</ul>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null">
<span class="text-node">Weak consistency models (not strong)</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="19" orig-style="null"><span class="text-node">Client-centric consistency models</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null"><span class="text-node">Causal consistency: strongest model available</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="19" orig-style="null"><span class="text-node">Eventual consistency models</span></li>
</ul>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Strong consistency models guarantee that the apparent order and visibility of updates is equivalent to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Note that this is by no means an exhaustive list. Again, consistency models are just arbitrary contracts between the programmer and system, so they can be almost anything.</span></p>
<h3 id="strong-consistency-models" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Strong consistency models</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Strong consistency models can further be divided into two similar, but slightly different consistency models:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null">
<em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="200.1875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Linearizable consistency</span></em><span class="text-node">: Under linearizable consistency, all operations </span><strong data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="61.6875" data-pf_rect_height="19" orig-style="null"><span class="text-node">appear</span></strong><span class="text-node"> to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy &amp; Wing, 1991)</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null">
<em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="186.53125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Sequential consistency</span></em><span class="text-node">: Under sequential consistency, all operations </span><strong data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="61.6875" data-pf_rect_height="19" orig-style="null"><span class="text-node">appear</span></strong><span class="text-node"> to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)</span>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">The key difference is that linearizable consistency requires that the order in which operations take effect is equal to the actual real-time ordering of operations. Sequential consistency allows for operations to be reordered as long as the order observed on each node remains consistent. The only way someone can distinguish between the two is if they can observe all the inputs and timings going into the system; from the perspective of a client interacting with a node, the two are equivalent.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">The difference seems immaterial, but it is worth noting that sequential consistency does not compose.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Strong consistency models allow you as a programmer to replace a single server with a cluster of distributed nodes and not run into any problems.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">All the other consistency models have anomalies (compared to a system that guarantees strong consistency), because they behave in a way that is distinguishable from a non-replicated system. But often these anomalies are acceptable, either because we don't care about occasional issues or because we've written code that deals with inconsistencies after they have occurred in some way.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Note that there really aren't any universal typologies for weak consistency models, because "not a strong consistency model" (e.g. "is distinguishable from a non-replicated system in some way") can be almost anything.</span></p>
<h3 id="client-centric-consistency-models" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Client-centric consistency models</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="273.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Client-centric consistency models</span></em><span class="text-node"> are consistency models that involve the notion of a client or session in some way. For example, a client-centric consistency model might guarantee that a client will never see older versions of a data item. This is often implemented by building additional caching into the client library, so that if a client moves to a replica node that contains old data, then the client library returns its cached value rather than the old value from the replica.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Clients may still see older versions of the data, if the replica node they are on does not contain the latest version, but they will never see anomalies where an older version of a value resurfaces (e.g. because they connected to a different replica). Note that there are many kinds of consistency models that are client-centric.</span></p>
<h3 id="eventual-consistency" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Eventual consistency</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">The </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="169.265625" data-pf_rect_height="19" orig-style="null"><span class="text-node">eventual consistency</span></em><span class="text-node"> model says that if you stop changing values, then after some undefined amount of time all replicas will agree on the same value. It is implied that before that time results between replicas are inconsistent in some undefined manner. Since it is </span><a href="http://www.bailis.org/blog/safety-and-liveness-eventual-consistency-is-not-safe/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="150.015625" data-pf_rect_height="19" orig-style="null"><span class="text-node">trivially satisfiable</span></a><span class="text-node"> (liveness property only), it is useless without supplemental information.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Saying something is merely eventually consistent is like saying "people are eventually dead". It's a very weak constraint, and we'd probably want to have at least some more specific characterization of two things:</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">First, how long is "eventually"? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Second, how do the replicas agree on a value? A system that always returns "42" is eventually consistent: all replicas agree on the same value. It just doesn't converge to a useful value since it just keeps returning the same fixed value. Instead, we'd like to have a better idea of the method. For example, one way to decide is to have the value with the largest timestamp always win.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">So when vendors say "eventual consistency", what they mean is some more precise term, such as "eventually last-writer-wins, and read-the-latest-observed-value in the meantime" consistency. The "how?" matters, because a bad method can lead to writes being lost - for example, if the clock on one node is set incorrectly and timestamps are used.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">I will look into these two questions in more detail in the chapter on replication methods for weak consistency models.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h2 id="further-reading" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Further reading</span></h2>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="380" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="342.34375" data-pf_rect_height="57" orig-style="null"><span class="text-node">Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services</span></a><span class="text-node"> - Gilbert &amp; Lynch, 2002</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="307.578125" data-pf_rect_height="38" orig-style="null"><span class="text-node">Impossibility of distributed consensus with one faulty process</span></a><span class="text-node"> - Fischer, Lynch and Patterson, 1985</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Perspectives+on+the+CAP+Theorem" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="275.5625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Perspectives on the CAP Theorem</span></a><span class="text-node"> - Gilbert &amp; Lynch, 2012</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="338.265625" data-pf_rect_height="38" orig-style="null"><span class="text-node">CAP Twelve Years Later: How the "Rules" Have Changed</span></a><span class="text-node"> - Brewer, 2012</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Uniform+consensus+is+harder+than+consensus" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="275.78125" data-pf_rect_height="38" orig-style="null"><span class="text-node">Uniform consensus is harder than consensus</span></a><span class="text-node"> - Charron-Bost &amp; Schiper, 2000</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://pages.cs.wisc.edu/~remzi/Classes/739/Papers/Bart/ConsistencyAndBaseballReport.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="318.640625" data-pf_rect_height="38" orig-style="null"><span class="text-node">Replicated Data Consistency Explained Through Baseball</span></a><span class="text-node"> - Terry, 2011</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Life+Beyond+Distributed+Transactions%3A+an+Apostate%27s+Opinion" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="333.515625" data-pf_rect_height="38" orig-style="null"><span class="text-node">Life Beyond Distributed Transactions: an Apostate's Opinion</span></a><span class="text-node"> - Helland, 2007</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://dl.acm.org/citation.cfm?id=1953140" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="306.84375" data-pf_rect_height="38" orig-style="null"><span class="text-node">If you have too much data, then 'good enough' is good enough</span></a><span class="text-node"> - Helland, 2011</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Building+on+Quicksand" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="183.640625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Building on Quicksand</span></a><span class="text-node"> - Helland &amp; Campbell, 2009</span>
</li>
</ul>

<h1 id="-chapter_number-time-and-order" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">3. Time and order</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">What is order and why is it important?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">What do you mean "what is order"?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">I mean, why are we so obsessed with order in the first place? Why do we care whether A happened before B? Why don't we care about some other property, like "color"?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Well, my crazy friend, let's go back to the definition of distributed systems to answer that.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">As you may remember, I described distributed programming as the art of solving the same problem that you can solve on a single computer using multiple computers.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">This is, in fact, at the core of the obsession with order. Any system that can only do one thing at a time will create a total order of operations. Like people passing through a single door, every operation will have a well-defined predecessor and successor. That's basically the programming model that we've worked very hard to preserve.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="266" class="added-to-list1" orig-style="null"><span class="text-node">The traditional model is: a single program, one process, one memory space running on one CPU. The operating system abstracts away the fact that there might be multiple CPUs and multiple programs, and that the memory on the computer is actually shared among many programs. I'm not saying that threaded programming and event-oriented programming don't exist; it's just that they are special abstractions on top of the "one/one/one" model. Programs are written to be executed in an ordered fashion: you start from the top, and then go down towards the bottom.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Order as a property has received so much attention because the easiest way to define "correctness" is to say "it works like it would on a single machine". And that usually means that a) we run the same operations and b) that we run them in the same order - even if there are multiple machines.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The nice thing about distributed systems that preserve order (as defined for a single system) is that they are generic. You don't need to care about what the operations are, because they will be executed exactly like on a single machine. This is great because you know that you can use the same system no matter what the operations are.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">In reality, a distributed program runs on multiple nodes; with multiple CPUs and multiple streams of operations coming in. You can still assign a total order, but it requires either accurate clocks or some form of communication. You could timestamp each operation using a completely accurate clock then use that to figure out the total order. Or you might have some kind of communication system that makes it possible to assign sequential numbers as in a total order.</span></p>
<h2 id="total-and-partial-order" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Total and partial order</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The natural state in a distributed system is </span><a href="http://en.wikipedia.org/wiki/Partially_ordered_set" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="103.265625" data-pf_rect_height="19" orig-style="null"><span class="text-node">partial order</span></a><span class="text-node">. Neither the network nor independent nodes make any guarantees about relative order; but at each node, you can observe a local order.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">A </span><a href="http://en.wikipedia.org/wiki/Total_order" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="86.796875" data-pf_rect_height="19" orig-style="null"><span class="text-node">total order</span></a><span class="text-node"> is a binary relation that defines an order for every element in some set.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Two distinct elements are </span><strong data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="105.09375" data-pf_rect_height="19" orig-style="null"><span class="text-node">comparable</span></strong><span class="text-node"> when one of them is greater than the other. In a partially ordered set, some pairs of elements are not comparable and hence a partial order doesn't specify the exact order of every item.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Both total order and partial order are </span><a href="http://en.wikipedia.org/wiki/Transitive_relation" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="77.234375" data-pf_rect_height="19" orig-style="null"><span class="text-node">transitive</span></a><span class="text-node"> and </span><a href="http://en.wikipedia.org/wiki/Antisymmetric_relation" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="116.53125" data-pf_rect_height="19" orig-style="null"><span class="text-node">antisymmetric</span></a><span class="text-node">. The following statements hold in both a total order and a partial order for all a, b and c in X:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">If a ≤ b and b ≤ a then a = b (antisymmetry);
If a ≤ b and b ≤ c then a ≤ c (transitivity);</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">However, a total order is </span><a href="http://en.wikipedia.org/wiki/Total_relation" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="37.109375" data-pf_rect_height="19" orig-style="null"><span class="text-node">total</span></a><span class="text-node">:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="15" class="added-to-list1" orig-style="null">a ≤ b or b ≤ a (totality) for all a, b in X</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">while a partial order is only </span><a href="http://en.wikipedia.org/wiki/Reflexive_relation" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.234375" data-pf_rect_height="19" orig-style="null"><span class="text-node">reflexive</span></a><span class="text-node">:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="15" class="added-to-list1" orig-style="null">a ≤ a (reflexivity) for all a in X</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Note that totality implies reflexivity; so a partial order is a weaker variant of total order.
For some elements in a partial order, the totality property does not hold - in other words, some of the elements are not comparable.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Git branches are an example of a partial order. As you probably know, the git revision control system allows you to create multiple branches from a single base branch - e.g. from a master branch. Each branch represents a history of source code changes derived based on a common ancestor:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="45" class="added-to-list1" orig-style="null">[ branch A (1,2,0)]  [ master (3,0,0) ]  [ branch B (1,0,2) ]
[ branch A (1,1,0)]  [ master (2,0,0) ]  [ branch B (1,0,1) ]
                  \  [ master (1,0,0) ]  /</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">The branches A and B were derived from a common ancestor, but there is no definite order between them: they represent different histories and cannot be reduced to a single linear history without additional work (merging). You could, of course, put all the commits in some arbitrary order (say, sorting them first by ancestry and then breaking ties by sorting A before B or B before A) - but that would lose information by forcing a total order where none existed.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">In a system consisting of one node, a total order emerges by necessity: instructions are executed and messages are processed in a specific, observable order in a single program. We've come to rely on this total order - it makes executions of programs predictable. This order can be maintained on a distributed system, but at a cost: communication is expensive, and time synchronization is difficult and fragile.</span></p>
<h1 id="what-is-time-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">What is time?</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Time is a source of order - it allows us to define the order of operations - which coincidentally also has an interpretation that people can understand (a second, a minute, a day and so on).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">In some sense, time is just like any other integer counter. It just happens to be important enough that most computers have a dedicated time sensor, also known as a clock. It's so important that we've figured out how to synthesize an approximation of the same counter using some imperfect physical system (from wax candles to cesium atoms). By "synthesize", I mean that we can approximate the value of the integer counter in physically distant places via some physical property without communicating it directly.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially influenced by everything that happened before it. This idea can be generalized into a causal clock that explicitly tracks causes (dependencies) rather than simply assuming that everything that preceded a timestamp was relevant. Of course, the usual assumption is that we should only worry about the state of the specific system rather than the whole world.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Assuming that time progresses at the same rate everywhere - and that is a big assumption which I'll return to in a moment - time and timestamps have several useful interpretations when used in a program. The three interpretations are:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Order</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Duration</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Interpretation</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="48.09375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Order</span></em><span class="text-node">. When I say that time is a source of order, what I mean is that:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">we can attach timestamps to unordered events to order them</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">we can use timestamps to enforce a specific ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">we can use the value of a timestamp to determine whether something happened chronologically before something else</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="114.890625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Interpretation</span></em><span class="text-node"> - time as a universally comparable value. The absolute value of a timestamp can be interpreted as a date, which is useful for people. Given a timestamp of when a downtime started from a log file, you can tell that it was last Saturday, when there was a </span><a href="https://twitter.com/AWSFail/statuses/218915147060752384" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="111.71875" data-pf_rect_height="19" orig-style="null"><span class="text-node">thunderstorm</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="71.75" data-pf_rect_height="19" orig-style="null"><span class="text-node">Duration</span></em><span class="text-node"> - durations measured in time have some relation to the real world. Algorithms generally don't care about the absolute value of a clock or its interpretation as a date, but they might use durations to make some judgment calls. In particular, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">By their nature, the components of distributed systems do not behave in a predictable manner. They do not guarantee any specific order, rate of advance, or lack of delay. Each node does have some local order - as execution is (roughly) sequential - but these local orders are independent of each other.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Imposing (or assuming) order is one way to reduce the space of possible executions and possible occurrences. Humans have a hard time reasoning about things when things can happen in any order - there just are too many permutations to consider.</span></p>
<h2 id="does-time-progress-at-the-same-rate-everywhere-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Does time progress at the same rate everywhere?</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">We all have an intuitive concept of time based on our own experience as individuals. Unfortunately, that intuitive notion of time makes it easier to picture total order rather than partial order. It's easier to picture a sequence in which things happen one after another, rather than concurrently. It is easier to reason about a single order of messages than to reason about messages arriving in different orders and with different delays.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">However, when implementing distributing systems we want to avoid making strong assumptions about time and order, because the stronger the assumptions, the more fragile a system is to issues with the "time sensor" - or the onboard clock. Furthermore, imposing an order carries a cost. The more temporal nondeterminism that we can tolerate, the more we can take advantage of distributed computation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">There are three common answers to the question "does time progress at the same rate everywhere?". These are:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">"Global clock": yes</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">"Local clock": no, but</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">"No clock": no!</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">These correspond roughly to the three timing assumptions that I mentioned in the second chapter: the synchronous system model has a global clock, the partially synchronous model has a local clock, and in the asynchronous system model one cannot use clocks at all. Let's look at these in more detail.</span></p>
<h3 id="time-with-a-global-clock-assumption" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Time with a "global-clock" assumption</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">The global clock assumption is that there is a global clock of perfect accuracy, and that everyone has access to that clock. This is the way we tend to think about time, because in human interactions small differences in time don't really matter.</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f676c6f62616c2d636c6f636b2e706e67" alt="Global clock" pf-orig-src="http://book.mixu.net/distsys/images/global-clock.png" pf-restore-src="https://pdf.printfriendly.com/camo/9941bae054a1f898ce1fbee5544b28599ac4b6cf/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f676c6f62616c2d636c6f636b2e706e67" pf-data-width="346" pf-data-height="405" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="104" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full mediumImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The global clock is basically a source of total order (exact order of every operation on all nodes even if those nodes have never communicated).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">However, this is an idealized view of the world: in reality, clock synchronization is only possible to a limited degree of accuracy. This is limited by the lack of accuracy of clocks in commodity computers, by latency if a clock synchronization protocol such as </span><a href="http://en.wikipedia.org/wiki/Network_Time_Protocol" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="35.4375" data-pf_rect_height="19" orig-style="null"><span class="text-node">NTP</span></a><span class="text-node"> is used and fundamentally by </span><a href="http://en.wikipedia.org/wiki/Time_dilation" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="193.1875" data-pf_rect_height="19" orig-style="null"><span class="text-node">the nature of spacetime</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="285" class="added-to-list1" orig-style="null"><span class="text-node">Assuming that clocks on distributed nodes are perfectly synchronized means assuming that clocks start at the same value and never drift apart. It's a nice assumption because you can use timestamps freely to determine a global total order - bound by clock drift rather than latency - but this is a </span><a href="http://queue.acm.org/detail.cfm?id=1773943" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="78.171875" data-pf_rect_height="19" orig-style="null"><span class="text-node">nontrivial</span></a><span class="text-node"> operational challenge and a potential source of anomalies. There are many different scenarios where a simple failure - such as a user accidentally changing the local time on a machine, or an out-of-date machine joining a cluster, or synchronized clocks drifting at slightly different rates and so on that can cause hard-to-trace anomalies.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="285" class="added-to-list1" orig-style="null"><span class="text-node">Nevertheless, there are some real-world systems that make this assumption. Facebook's </span><a href="http://en.wikipedia.org/wiki/Apache_Cassandra" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="85.40625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Cassandra</span></a><span class="text-node"> is an example of a system that assumes clocks are synchronized. It uses timestamps to resolve conflicts between writes - the write with the newer timestamp wins. This means that if clocks drift, new data may be ignored or overwritten by old data; again, this is an operational challenge (and from what I've heard, one that people are acutely aware of). Another interesting example is Google's </span><a href="http://research.google.com/archive/spanner.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="68.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Spanner</span></a><span class="text-node">: the paper describes their TrueTime API, which synchronizes time but also estimates worst-case clock drift.</span></p>
<h3 id="time-with-a-local-clock-assumption" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Time with a "Local-clock" assumption</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The second, and perhaps more plausible assumption is that each machine has its own clock, but there is no global clock. It means that you cannot use the local clock in order to determine whether a remote timestamp occurred before or after a local timestamp; in other words, you cannot meaningfully compare timestamps from two different machines.</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6c6f63616c2d636c6f636b2e706e67" alt="Local clock" pf-orig-src="http://book.mixu.net/distsys/images/local-clock.png" pf-restore-src="https://pdf.printfriendly.com/camo/1bdd72ac3c5d4c98f80d4145448d0fe572b33b53/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6c6f63616c2d636c6f636b2e706e67" pf-data-width="291" pf-data-height="399" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="96" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full mediumImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The local clock assumption corresponds more closely to the real world. It assigns a partial order: events on each system are ordered but events cannot be ordered across systems by only using a clock.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">However, you can use timestamps to order events on a single machine; and you can use timeouts on a single machine as long as you are careful not to allow the clock to jump around. Of course, on a machine controlled by an end-user this is probably assuming too much: for example, a user might accidentally change their date to a different value while looking up a date using the operating system's date control.</span></p>
<h3 id="time-with-a-no-clock-assumption" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Time with a "No-clock" assumption</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Finally, there is the notion of logical time. Here, we don't use clocks at all and instead track causality in some other way. Remember, a timestamp is simply a shorthand for the state of the world up to that point - so we can use counters and communication to determine whether something happened before, after or concurrently with something else.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This way, we can determine the order of events between different machines, but cannot say anything about intervals and cannot use timeouts (since we assume that there is no "time sensor"). This is a partial order: events can be ordered on a single system using a counter and no communication, but ordering events across systems requires a message exchange.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">One of the most cited papers in distributed systems is Lamport's paper on </span><a href="http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="349.140625" data-pf_rect_height="38" orig-style="null"><span class="text-node">time, clocks and the ordering of events</span></a><span class="text-node">. Vector clocks, a generalization of that concept (which I will cover in more detail), are a way to track causality without using clocks. Cassandra's cousins Riak (Basho) and Voldemort (Linkedin) use vector clocks rather than assuming that nodes have access to a global clock of perfect accuracy. This allows those systems to avoid the clock accuracy issues mentioned earlier.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">When clocks are not used, the maximum precision at which events can be ordered across distant machines is bound by communication latency.</span></p>
<h2 id="how-is-time-used-in-a-distributed-system-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">How is time used in a distributed system?</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">What is the benefit of time?</span></p>
<ol class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Time can define order across a system (without communication)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Time can define boundary conditions for algorithms</span></li>
</ol>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The order of events is important in distributed systems, because many properties of distributed systems are defined in terms of the order of operations/events:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">where correctness depends on (agreement on) correct event ordering, for example serializability in a distributed database</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">order can be used as a tie breaker when resource contention occurs, for example if there are two orders for a widget, fulfill the first and cancel the second one</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">A global clock would allow operations on two different machines to be ordered without the two machines communicating directly. Without a global clock, we need to communicate in order to determine order.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Time can also be used to define boundary conditions for algorithms - specifically, to distinguish between "high latency" and "server or network link is down". This is a very important use case; in most real-world systems timeouts are used to determine whether a remote machine has failed, or whether it is simply experiencing high network latency. Algorithms that make this determination are called failure detectors; and I will discuss them fairly soon.</span></p>
<h2 id="vector-clocks-time-for-causal-order-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Vector clocks (time for causal order)</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Earlier, we discussed the different assumptions about the rate of progress of time across a distributed system. Assuming that we cannot achieve accurate clock synchronization - or starting with the goal that our system should not be sensitive to issues with time synchronization, how can we order things?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Lamport clocks and vector clocks are replacements for physical clocks which rely on counters and communication to determine the order of events across a distributed system. These clocks provide a counter that is comparable across different nodes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="133.28125" data-pf_rect_height="19" orig-style="null"><span class="text-node">A Lamport clock</span></em><span class="text-node"> is simple. Each process maintains a counter using the following rules:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Whenever a process does work, increment the counter</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Whenever a process sends a message, include the counter</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<span class="text-node">When a message is received, set the counter to </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="229.53125" data-pf_rect_height="34" orig-style="null">max(local_counter, received_counter) + 1</code>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">Expressed as code:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="225" class="added-to-list1" orig-style="null">function LamportClock() {
  this.value = 1;
}

LamportClock.prototype.get = function() {
  return this.value;
}

LamportClock.prototype.increment = function() {
  this.value++;
}

LamportClock.prototype.merge = function(other) {
  this.value = Math.max(this.value, other.value) + 1;
}</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">A </span><a href="http://en.wikipedia.org/wiki/Lamport_timestamps" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="116.65625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Lamport clock</span></a><span class="text-node"> allows counters to be compared across systems, with a caveat: Lamport clocks define a partial order. If </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="210.9375" data-pf_rect_height="15" orig-style="null">timestamp(a) &lt; timestamp(b)</code><span class="text-node">:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">a</code><span class="text-node"> may have happened before </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">b</code><span class="text-node"> or</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">a</code><span class="text-node"> may be incomparable with </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">b</code>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">This is known as clock consistency condition: if one event comes before another, then that event's logical clock comes before the others. If </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">a</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">b</code><span class="text-node"> are from the same causal history, e.g. either both timestamp values were produced on the same process; or </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">b</code><span class="text-node"> is a response to the message sent in </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">a</code><span class="text-node"> then we know that </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">a</code><span class="text-node"> happened before </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">b</code><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Intuitively, this is because a Lamport clock can only carry information about one timeline / history; hence, comparing Lamport timestamps from systems that never communicate with each other may cause concurrent events to appear to be ordered when they are not.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Imagine a system that after an initial period divides into two independent subsystems which never communicate with each other.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">For all events in each independent system, if a happened before b, then </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="101.5625" data-pf_rect_height="15" orig-style="null">ts(a) &lt; ts(b)</code><span class="text-node">; but if you take two events from the different independent systems (e.g. events that are not causally related) then you cannot say anything meaningful about their relative order.  While each part of the system has assigned timestamps to events, those timestamps have no relation to each other. Two events may appear to be ordered even though they are unrelated.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">However - and this is still a useful property - from the perspective of a single machine, any message sent with </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">ts(a)</code><span class="text-node"> will receive a response with </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">ts(b)</code><span class="text-node"> which is </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="54.6875" data-pf_rect_height="15" orig-style="null">&gt; ts(a)</code><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="115.171875" data-pf_rect_height="19" orig-style="null"><span class="text-node">A vector clock</span></em><span class="text-node"> is an extension of Lamport clock, which maintains an array </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="117.1875" data-pf_rect_height="15" orig-style="null">[ t1, t2, ... ]</code><span class="text-node"> of N logical clocks - one per each node. Rather than incrementing a common counter, each node increments its own logical clock in the vector by one on each internal event. Hence the update rules are:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">Whenever a process does work, increment the logical clock value of the node in the vector</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Whenever a process sends a message, include the full vector of logical clocks</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null">
<span class="text-node">When a message is received:</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null">
<span class="text-node">update each element in the vector to be </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="156.25" data-pf_rect_height="15" orig-style="null">max(local, received)</code>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">increment the logical clock value representing the current node in the vector</span></li>
</ul>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">Again, expressed as code:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="510" class="added-to-list1" orig-style="null">function VectorClock(value) {
  // expressed as a hash keyed by node id: e.g. { node1: 1, node2: 3 }
  this.value = value || {};
}

VectorClock.prototype.get = function() {
  return this.value;
};

VectorClock.prototype.increment = function(nodeId) {
  if(typeof this.value[nodeId] == 'undefined') {
    this.value[nodeId] = 1;
  } else {
    this.value[nodeId]++;
  }
};

VectorClock.prototype.merge = function(other) {
  var result = {}, last,
      a = this.value,
      b = other.value;
  // This filters out duplicate keys in the hash
  (Object.keys(a)
    .concat(b))
    .sort()
    .filter(function(key) {
      var isDuplicate = (key == last);
      last = key;
      return !isDuplicate;
    }).forEach(function(key) {
      result[key] = Math.max(a[key] || 0, b[key] || 0);
    });
  this.value = result;
};</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">This illustration (</span><a href="http://en.wikipedia.org/wiki/Vector_clock" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="54.1875" data-pf_rect_height="19" orig-style="null"><span class="text-node">source</span></a><span class="text-node">) shows a vector clock:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f766563746f725f636c6f636b2e7376672e706e67" alt="from http://en.wikipedia.org/wiki/Vector_clock" pf-orig-src="http://book.mixu.net/distsys/images/vector_clock.svg.png" pf-restore-src="https://pdf.printfriendly.com/camo/25e3c2150081ed1d3e3cc36a7aeefe8f03b3524e/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f766563746f725f636c6f636b2e7376672e706e67" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="380" data-pf_rect_height="23" orig-style="null" srcset="" class="flex-width pf-size-full blockImage"></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Each of the three nodes (A, B, C) keeps track of the vector clock. As events occur, they are timestamped with the current value of the vector clock. Examining a vector clock such as </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="156.25" data-pf_rect_height="15" orig-style="null">{ A: 2, B: 4, C: 1 }</code><span class="text-node"> lets us accurately identify the messages that (potentially) influenced that event.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The issue with vector clocks is mainly that they require one entry per node, which means that they can potentially become very large for large systems. A variety of techniques have been applied to reduce the size of vector clocks (either by performing periodic garbage collection, or by reducing accuracy by limiting the size).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">We've looked at how order and causality can be tracked without physical clocks. Now, let's look at how time durations can be used for cutoff.</span></p>
<h2 id="failure-detectors-time-for-cutoff-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Failure detectors (time for cutoff)</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">As I stated earlier, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency. In this case, we don't need to assume a global clock of perfect accuracy - it is simply enough that there is a reliable-enough local clock.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Given a program running on one node, how can it tell that a remote node has failed? In the absence of accurate information, we can infer that an unresponsive remote node has failed after some reasonable amount of time has passed.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">But what is a "reasonable amount"? This depends on the latency between the local and remote nodes. Rather than explicitly specifying algorithms with specific values (which would inevitably be wrong in some cases), it would be nicer to deal with a suitable abstraction.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">A failure detector is a way to abstract away the exact timing assumptions. Failure detectors are implemented using heartbeat messages and timers. Processes exchange heartbeat messages. If a message response is not received before the timeout occurs, then the process suspects the other process.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">A failure detector based on a timeout will carry the risk of being either overly aggressive (declaring a node to have failed) or being overly conservative (taking a long time to detect a crash). How accurate do failure detectors need to be for them to be usable?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><a href="http://www.google.com/search?q=Unreliable%20Failure%20Detectors%20for%20Reliable%20Distributed%20Systems" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="115.53125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Chandra et al.</span></a><span class="text-node"> (1996) discuss failure detectors in the context of solving consensus - a problem that is particularly relevant since it underlies most replication problems where the replicas need to agree in environments with latency and network partitions.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">They characterize failure detectors using two properties, completeness and accuracy:</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Strong completeness.</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Every crashed process is eventually suspected by every correct process.</span></dd>
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Weak completeness.</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Every crashed process is eventually suspected by some correct process.</span></dd>
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Strong accuracy.</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">No correct process is suspected ever.</span></dd>
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Weak accuracy.</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Some correct process is never suspected.</span></dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Completeness is easier to achieve than accuracy; indeed, all failure detectors of importance achieve it - all you need to do is not to wait forever to suspect someone. Chandra et al. note that a failure detector with weak completeness can be transformed to one with strong completeness (by broadcasting information about suspected processes), allowing us to concentrate on the spectrum of accuracy properties.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Avoiding incorrectly suspecting non-faulty processes is hard unless you are able to assume that there is a hard maximum on the message delay. That assumption can be made in a synchronous system model - and hence failure detectors can be strongly accurate in such a system. Under system models that do not impose hard bounds on message delay, failure detection can at best be eventually accurate.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Chandra et al. show that even a very weak failure detector - the eventually weak failure detector ⋄W (eventually weak accuracy + weak completeness) - can be used to solve the consensus problem. The diagram below (from the paper) illustrates the relationship between system models and problem solvability:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6368616e6472615f6661696c7572655f6465746563746f72732e706e67" alt="From Chandra and Toueg. Unreliable failure detectors for reliable distributed systems. JACM 43(2):225–267, 1996." pf-orig-src="http://book.mixu.net/distsys/images/chandra_failure_detectors.png" pf-restore-src="https://pdf.printfriendly.com/camo/b2875ac48032ec69705e68034e884bc7df123d29/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f6368616e6472615f6661696c7572655f6465746563746f72732e706e67" pf-data-width="487" pf-data-height="289" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="942" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">As you can see above, certain problems are not solvable without a failure detector in asynchronous systems. This is because without a failure detector (or strong assumptions about time bounds e.g. the synchronous system model), it is not possible to tell whether a remote node has crashed, or is simply experiencing high latency. That distinction is important for any system that aims for single-copy consistency: failed nodes can be ignored because they cannot cause divergence, but partitioned nodes cannot be safely ignored.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">How can one implement a failure detector? Conceptually, there isn't much to a simple failure detector, which simply detects failure when a timeout expires. The most interesting part relates to how the judgments are made about whether a remote node has failed.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Ideally, we'd prefer the failure detector to be able to adjust to changing network conditions and to avoid hardcoding timeout values into it. For example, Cassandra uses an </span><a href="https://www.google.com/search?q=The+Phi+accrual+failure+detector" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="327.375" data-pf_rect_height="38" orig-style="null"><span class="text-node">accrual failure detector</span></a><span class="text-node">, which is a failure detector that outputs a suspicion level (a value between 0 and 1) rather than a binary "up" or "down" judgment. This allows the application using the failure detector to make its own decisions about the tradeoff between accurate detection and early detection.</span></p>
<h2 id="time-order-and-performance" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Time, order and performance</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Earlier, I alluded to having to pay the cost for order. What did I mean?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">If you're writing a distributed system, you presumably own more than one computer. The natural (and realistic) view of the world is a partial order, not a total order. You can transform a partial order into a total order, but this requires communication, waiting and imposes restrictions that limit how many computers can do work at any particular point in time.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">All clocks are mere approximations bound by either network latency (logical time) or by physics. Even keeping a simple integer counter in sync across multiple nodes is a challenge.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">While time and order are often discussed together, time itself is not such a useful property. Algorithms don't really care about time as much as they care about more abstract properties:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">the causal ordering of events</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">failure detection (e.g. approximations of upper bounds on message delivery)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">consistent snapshots (e.g. the ability to examine the state of a system at some point in time; not discussed here)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Imposing a total order is possible, but expensive. It requires you to proceed at the common (lowest) speed. Often the easiest way to ensure that events are delivered in some defined order is to nominate a single (bottleneck) node through which all operations are passed.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Is time / order / synchronicity really necessary? It depends. In some use cases, we want each intermediate operation to move the system from one consistent state to another. For example, in many cases we want the responses from a database to represent all of the available information, and we want to avoid dealing with the issues that might occur if the system could return an inconsistent result.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">But in other cases, we might not need that much time / order / synchronization. For example, if you are running a long running computation, and don't really care about what the system does until the very end - then you don't really need much synchronization as long as you can guarantee that the answer is correct.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Synchronization is often applied as a blunt tool across all operations, when only a subset of cases actually matter for the final outcome. When is order needed to guarantee correctness? The CALM theorem - which I will discuss in the last chapter - provides one answer.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="304" class="added-to-list1" orig-style="null"><span class="text-node">In other cases, it is acceptable to give an answer that only represents the best known estimate - that is, is based on only a subset of the total information contained in the system. In particular, during a network partition one may need to answer queries with only a part of the system being accessible. In other use cases, the end user cannot really distinguish between a relatively recent answer that can be obtained cheaply and one that is guaranteed to be correct and is expensive to calculate. For example, is the Twitter follower count for some user X, or X+1? Or are movies A, B and C the absolutely best answers for some query? Doing a cheaper, mostly correct "best effort" can be acceptable.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">In the next two chapters we'll examine replication for fault-tolerant strongly consistent systems - systems which provide strong guarantees while being increasingly resilient to failures. These systems provide solutions for the first case: when you need to guarantee correctness and are willing to pay for it. Then, we'll discuss systems with weak consistency guarantees, which can remain available in the face of partitions, but that can only give you a "best effort" answer.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h2 id="further-reading" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Further reading</span></h2>
<h3 id="lamport-clocks-vector-clocks" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Lamport clocks, vector clocks</span></h3>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="334.234375" data-pf_rect_height="38" orig-style="null"><span class="text-node">Time, Clocks and Ordering of Events in a Distributed System</span></a><span class="text-node"> - Leslie Lamport, 1978</span>
</li>
</ul>
<h3 id="failure-detection" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Failure detection</span></h3>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Unreliable+Failure+Detectors+for+Reliable+Distributed+Systems" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="327.015625" data-pf_rect_height="38" orig-style="null"><span class="text-node">Unreliable failure detectors and reliable distributed systems</span></a><span class="text-node"> - Chandra and Toueg</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://www.cs.cornell.edu/people/egs/sqrt-s/doc/TR2006-2025.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="294.15625" data-pf_rect_height="38" orig-style="null"><span class="text-node">Latency- and Bandwidth-Minimizing Optimal Failure Detectors</span></a><span class="text-node"> - So &amp; Sirer, 2007</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://scholar.google.com/scholar?q=The+failure+detector+abstraction" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="258.953125" data-pf_rect_height="19" orig-style="null"><span class="text-node">The failure detector abstraction</span></a><span class="text-node">, Freiling, Guerraoui &amp; Kuznetsov, 2011</span>
</li>
</ul>
<h3 id="snapshots" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Snapshots</span></h3>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Consistent+global+states+of+distributed+systems%3A+Fundamental+concepts+and+mechanisms" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="309.609375" data-pf_rect_height="57" orig-style="null"><span class="text-node">Consistent global states of distributed systems: Fundamental concepts and mechanisms</span></a><span class="text-node">, Ozalp Babaogly and Keith Marzullo, 1993</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Distributed+snapshots%3A+Determining+global+states+of+distributed+systems" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="290.578125" data-pf_rect_height="38" orig-style="null"><span class="text-node">Distributed snapshots: Determining global states of distributed systems</span></a><span class="text-node">, K. Mani Chandy and Leslie Lamport, 1985</span>
</li>
</ul>
<h3 id="causality" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Causality</span></h3>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://www.vs.inf.ethz.ch/publ/papers/holygrail.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="319.84375" data-pf_rect_height="57" orig-style="null"><span class="text-node">Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail</span></a><span class="text-node"> - Schwarz &amp; Mattern, 1994</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Understanding+the+limitations+of+causally+and+totally+ordered+communication" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="270.046875" data-pf_rect_height="57" orig-style="null"><span class="text-node">Understanding the Limitations of Causally and Totally Ordered Communication</span></a><span class="text-node"> - Cheriton &amp; Skeen, 1993</span>
</li>
</ul>

<h1 id="-chapter_number-replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">4. Replication</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">The replication problem is one of many problems in distributed systems. I've chosen to focus on it over other problems such as leader election, failure detection, mutual exclusion, consensus and global snapshots because it is often the part that people are most interested in. One way in which parallel databases are differentiated is in terms of their replication features, for example. Furthermore, replication provides a context for many subproblems, such as leader election, failure detection, consensus and atomic broadcast.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Replication is a group communication problem. What arrangement and communication pattern gives us the performance and availability characteristics we desire? How can we ensure fault tolerance, durability and non-divergence in the face of network partitions and simultaneous node failure?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Again, there are many ways to approach replication. The approach I'll take here just looks at high level patterns that are possible for a system with replication. Looking at this visually helps keep the discussion focused on the overall pattern rather than the specific messaging involved. My goal here is to explore the design space rather than to explain the specifics of each algorithm.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Let's first define what replication looks like. We assume that we have some initial database, and that clients make requests which change the state of the database.</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="344" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d626f74682e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/replication-both.png" pf-restore-src="https://pdf.printfriendly.com/camo/d8df657b64d8422b99e7253b89b31615eeba6a93/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d626f74682e706e67" pf-data-width="1130" pf-data-height="779" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="1360" data-pf_rect_height="340" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">The arrangement and communication pattern can then be divided into several stages:</span></p>
<ol class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">(Request) The client sends a request to a server</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">(Sync) The synchronous portion of the replication takes place</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">(Response) A response is returned to the client</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">(Async) The asynchronous portion of the replication takes place</span></li>
</ol>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">This model is loosely based on </span><a href="https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="87.34375" data-pf_rect_height="19" orig-style="null"><span class="text-node">this article</span></a><span class="text-node">. Note that the pattern of messages exchanged in each portion of the task depends on the specific algorithm: I am intentionally trying to get by without discussing the specific algorithm.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Given these stages, what kind of communication patterns can we create? And what are the performance and availability implications of the patterns we choose?</span></p>
<h2 id="synchronous-replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Synchronous replication</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The first pattern is synchronous replication (also known as active, or eager, or push, or pessimistic replication). Let's draw what that looks like:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="344" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d73796e632e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/replication-sync.png" pf-restore-src="https://pdf.printfriendly.com/camo/43c4585088da46524f43823d36d81dd4843723cf/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d73796e632e706e67" pf-data-width="852" pf-data-height="779" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="1360" data-pf_rect_height="340" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Here, we can see three distinct stages: first, the client sends the request. Next, what we called the synchronous portion of replication takes place. The term refers to the fact that the client is blocked - waiting for a reply from the system.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">During the synchronous phase, the first server contacts the two other servers and waits until it has received replies from all the other servers. Finally, it sends a response to the client informing it of the result (e.g. success or failure).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">All this seems straightforward. What can we say of this specific arrangement of communication patterns, without discussing the details of the algorithm during the synchronous phase? First, observe that this is a write N - of - N approach: before a response is returned, it has to be seen and acknowledged by every server in the system.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">From a performance perspective, this means that the system will be as fast as the slowest server in it. The system will also be very sensitive to changes in network latency, since it requires every server to reply before proceeding.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Given the N-of-N approach, the system cannot tolerate the loss of any servers. When a server is lost, the system can no longer write to all the nodes, and so it cannot proceed. It might be able to provide read-only access to the data, but modifications are not allowed after a node has failed in this design.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">This arrangement can provide very strong durability guarantees: the client can be certain that all N servers have received, stored and acknowledged the request when the response is returned. In order to lose an accepted update, all N copies would need to be lost, which is about as good a guarantee as you can make.</span></p>
<h2 id="asynchronous-replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Asynchronous replication</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Let's contrast this with the second pattern - asynchronous replication (a.k.a. passive replication, or pull replication, or lazy replication). As you may have guessed, this is the opposite of synchronous replication:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="344" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d6173796e632e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/replication-async.png" pf-restore-src="https://pdf.printfriendly.com/camo/309412d1cafc61dfa71e04398a66dd0cad906c2a/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7265706c69636174696f6e2d6173796e632e706e67" pf-data-width="852" pf-data-height="749" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="1360" data-pf_rect_height="340" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Here, the master (/leader / coordinator) immediately sends back a response to the client. It might at best store the update locally, but it will not do any significant work synchronously and the client is not forced to wait for more rounds of communication to occur between the servers.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">At some later stage, the asynchronous portion of the replication task takes place. Here, the master contacts the other servers using some communication pattern, and the other servers update their copies of the data. The specifics depend on the algorithm in use.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">What can we say of this specific arrangement without getting into the details of the algorithm? Well, this is a write 1 - of - N approach: a response is returned immediately and update propagation occurs sometime later.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">From a performance perspective, this means that the system is fast: the client does not need to spend any additional time waiting for the internals of the system to do their work. The system is also more tolerant of network latency, since fluctuations in internal latency do not cause additional waiting on the client side.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">This arrangement can only provide weak, or probabilistic durability guarantees. If nothing goes wrong, the data is eventually replicated to all N machines. However, if the only server containing the data is lost before this can take place, the data is permanently lost.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Given the 1-of-N approach, the system can remain available as long as at least one node is up (at least in theory, though in practice the load will probably be too high). A purely lazy approach like this provides no durability or consistency guarantees; you may be allowed to write to the system, but there are no guarantees that you can read back what you wrote if any faults occur.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Finally, it's worth noting that passive replication cannot ensure that all nodes in the system always contain the same state. If you accept writes at multiple locations and do not require that those nodes synchronously agree, then you will run the risk of divergence: reads may return different results from different locations (particularly after nodes fail and recover), and global constraints (which require communicating with everyone) cannot be enforced.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">I haven't really mentioned the communication patterns during a read (rather than a write), because the pattern of reads really follows from the pattern of writes: during a read, you want to contact as few nodes as possible. We'll discuss this a bit more in the context of quorums.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">We've only discussed two basic arrangements and none of the specific algorithms. Yet we've been able to figure out quite a bit of about the possible communication patterns as well as their performance, durability guarantees and availability characteristics.</span></p>
<h2 id="an-overview-of-major-replication-approaches" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">An overview of major replication approaches</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Having discussed the two basic replication approaches: synchronous and asynchronous replication, let's have a look at the major replication algorithms.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">There are many, many different ways to categorize replication techniques. The second distinction (after sync vs. async) I'd like to introduce is between:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Replication methods that prevent divergence (single copy systems) and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Replication methods that risk divergence (multi-master systems)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The first group of methods has the property that they "behave like a single system". In particular, when partial failures occur, the system ensures that only a single copy of the system is active. Furthermore, the system ensures that the replicas are always in agreement. This is known as the consensus problem.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Several processes (or computers) achieve consensus if they all agree on some value. More formally:</span></p>
<ol class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Agreement: Every correct process must agree on the same value.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Termination: All processes eventually reach a decision.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">Validity: If all correct processes propose the same value V, then all correct processes decide V.</span></li>
</ol>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Mutual exclusion, leader election, multicast and atomic broadcast are all instances of the more general problem of consensus. Replicated systems that maintain single copy consistency need to solve the consensus problem in some way.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">The replication algorithms that maintain single-copy consistency include:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">1n messages (asynchronous primary/backup)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">2n messages (synchronous primary/backup)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">4n messages (2-phase commit, Multi-Paxos)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">6n messages (3-phase commit, Paxos with repeated leader election)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">These algorithms vary in their fault tolerance (e.g. the types of faults they can tolerate). I've classified these simply by the number of messages exchanged during an execution of the algorithm, because I think it is interesting to try to find an answer to the question "what are we buying with the added message exchanges?"</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">The diagram below, adapted from Ryan Barret at </span><a href="http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="56.84375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Google</span></a><span class="text-node">, describes some of the aspects of the different options:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f676f6f676c652d7472616e7361637430392e706e67" alt="Comparison of replication methods, from http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html" pf-orig-src="http://book.mixu.net/distsys/images/google-transact09.png" pf-restore-src="https://pdf.printfriendly.com/camo/e52d5e1042b53c2d2e3f92642fb0b03c0d77147b/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f676f6f676c652d7472616e7361637430392e706e67" pf-data-width="858" pf-data-height="454" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="1028" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">The consistency, latency, throughput, data loss and failover characteristics in the diagram above can really be traced back to the two different replication methods: synchronous replication (e.g. waiting before responding) and asynchronous replication. When you wait, you get worse performance but stronger guarantees. The throughput difference between 2PC and quorum systems will become apparent when we discuss partition (and latency) tolerance.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">In that diagram, algorithms enforcing weak (/eventual) consistency are lumped up into one category ("gossip"). However, I will discuss replication methods for weak consistency - gossip and (partial) quorum systems - in more detail. The "transactions" row really refers more to global predicate evaluation, which is not supported in systems with weak consistency (though local predicate evaluation can be supported).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">It is worth noting that systems enforcing weak consistency requirements have fewer generic algorithms, and more techniques that can be selectively applied. Since systems that do not enforce single-copy consistency are free to act like distributed systems consisting of multiple nodes, there are fewer obvious objectives to fix and the focus is more on giving people a way to reason about the characteristics of the system that they have.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">For example:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="323" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">Client-centric consistency models attempt to provide more intelligible consistency guarantees while allowing for divergence.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">CRDTs (convergent and commutative replicated datatypes) exploit semilattice properties (associativity, commutativity, idempotency) of certain state and operation-based data types.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">Confluence analysis (as in the Bloom language) uses information regarding the monotonicity of computations to maximally exploit disorder.</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">PBS (probabilistically bounded staleness) uses simulation and information collected from a real world system to characterize the expected behavior of partial quorum systems.</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">I'll talk about all of these a bit  further on, first; let's look at the replication algorithms that maintain single-copy consistency.</span></p>
<h2 id="primary-backup-replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Primary/backup replication</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Primary/backup replication (also known as primary copy replication master-slave replication or log shipping) is perhaps the most commonly used replication method, and the most basic algorithm. All updated are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas. There are two variants:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">asynchronous primary/backup replication and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">synchronous primary/backup replication</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The synchronous version requires two messages ("update" + "acknowledge receipt") while the asynchronous version could run with just one ("update").</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">P/B is very common. For example, by default MySQL replication uses the asynchronous variant. MongoDB also uses P/B (with some additional procedures for failover). All operations are performed on one master server, which serializes them to a local log, which is then replicated asynchronously to the backup servers.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">As we discussed earlier in the context of asynchronous replication, any asynchronous replication algorithm can only provide weak durability guarantees. In MySQL replication this manifests as replication lag: the asynchronous backups are always at least one operation behind the primary. If the primary fails, then the updates that have not yet been sent to the backups are lost.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The synchronous variant of primary/backup replication ensures that writes have been stored on other nodes before returning back to the client - at the cost of waiting for responses from other replicas. However, it is worth noting that even this variant can only offer weak guarantees. Consider the following simple failure scenario:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">the primary receives a write and sends it to the backup</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">the backup persists and ACKs the write</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">and then primary fails before sending ACK to the client</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The client now assumes that the commit failed, but the backup committed it; if the backup is promoted to primary, it will be incorrect. Manual cleanup may be needed to reconcile the failed primary or divergent backups.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">I am simplifying here of course. While all primary/backup replication algorithms follow the same general messaging pattern, they differ in their handling of failover, replicas being offline for extended periods and so on. However, it is not possible to be resilient to inopportune failures of the primary in this scheme.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">What is key in the log-shipping / primary/backup based schemes is that they can only offer a best-effort guarantee (e.g. they are susceptible to lost updates or incorrect updates if nodes fail at inopportune times). Furthermore, P/B schemes are susceptible to split-brain, where the failover to a backup kicks in due to a temporary network issue and causes both the primary and backup to be active at the same time.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">To prevent inopportune failures from causing consistency guarantees to be violated; we need to add another round of messaging, which gets us the two phase commit protocol (2PC).</span></p>
<h2 id="two-phase-commit-2pc-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Two phase commit (2PC)</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><a href="http://en.wikipedia.org/wiki/Two-phase_commit_protocol" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="151.75" data-pf_rect_height="19" orig-style="null"><span class="text-node">Two phase commit</span></a><span class="text-node"> (2PC) is a protocol used in many classic relational databases. For example, MySQL Cluster (not to be confused with the regular MySQL) provides synchronous replication using 2PC. The diagram below illustrates the message flow:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="75" class="added-to-list1" orig-style="null">[ Coordinator ] -&gt; OK to commit?     [ Peers ]
                &lt;- Yes / No

[ Coordinator ] -&gt; Commit / Rollback [ Peers ]
                &lt;- ACK</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">In the first phase (voting), the coordinator sends the update to all the participants. Each participant processes the update and votes whether to commit or abort. When voting to commit, the participants store the update onto a temporary area (the write-ahead log). Until the second phase completes, the update is considered temporary.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">In the second phase (decision), the coordinator decides the outcome and informs every participant about it. If all participants voted to commit, then the update is taken from the temporary area and made permanent.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Having a second phase in place before the commit is considered permanent is useful, because it allows the system to roll back an update when a node fails. In contrast, in primary/backup ("1PC"), there is no step for rolling back an operation that has failed on some nodes and succeeded on others, and hence the replicas could diverge.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">2PC is prone to blocking, since a single node failure (participant or coordinator) blocks progress until the node has recovered. Recovery is often possible thanks to the second phase, during which other nodes are informed about the system state. Note that 2PC assumes that the data in stable storage at each node is never lost and that no node crashes forever. Data loss is still possible if the data in the stable storage is corrupted in a crash.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">The details of the recovery procedures during node failures are quite complicated so I won't get into the specifics. The major tasks are ensuring that writes to disk are durable (e.g. flushed to disk rather than cached) and making sure that the right recovery decisions are made (e.g. learning the outcome of the round and then redoing or undoing an update locally).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">As we learned in the chapter regarding CAP, 2PC is a CA - it is not partition tolerant. The failure model that 2PC addresses does not include network partitions; the prescribed way to recover from a node failure is to wait until the network partition heals. There is no safe way to promote a new coordinator if one fails; rather a manual intervention is required. 2PC is also fairly latency-sensitive, since it is a write N-of-N approach in which writes cannot proceed until the slowest node acknowledges them.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">2PC strikes a decent balance between performance and fault tolerance, which is why it has been popular in relational databases. However, newer systems often use a partition tolerant consensus algorithm, since such an algorithm can provide automatic recovery from temporary network partitions as well as more graceful handling of increased between-node latency.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Let's look at partition tolerant consensus algorithms next.</span></p>
<h2 id="partition-tolerant-consensus-algorithms" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Partition tolerant consensus algorithms</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Partition tolerant consensus algorithms are as far as we're going to go in terms of fault-tolerant algorithms that maintain single-copy consistency. There is a further class of fault tolerant algorithms: algorithms that tolerate </span><a href="http://en.wikipedia.org/wiki/Byzantine_fault_tolerance" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="221.21875" data-pf_rect_height="19" orig-style="null"><span class="text-node">arbitrary (Byzantine) faults</span></a><span class="text-node">; these include nodes that fail by acting maliciously. Such algorithms are rarely used in commercial systems, because they are more expensive to run and more complicated to implement - and hence I will leave them out.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">When it comes to partition tolerant consensus algorithms, the most well-known algorithm is the Paxos algorithm. It is, however, notoriously difficult to implement and explain, so I will focus on Raft, a recent (~early 2013) algorithm designed to be easier to teach and implement. Let's first take a look at network partitions and the general characteristics of partition tolerant consensus algorithms.</span></p>
<h3 id="what-is-a-network-partition-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">What is a network partition?</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">A network partition is the failure of a network link to one or several nodes. The nodes themselves continue to stay active, and they may even be able to receive requests from clients on their side of the network partition. As we learned earlier - during the discussion of the CAP theorem - network partitions do occur and not all systems handle them gracefully.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Network partitions are tricky because during a network partition, it is not possible to distinguish between a failed remote node and the node being unreachable. If a network partition occurs but no nodes fail, then the system is divided into two partitions which are simultaneously active. The two diagrams below illustrate how a network partition can look similar to a node failure.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">A system of 2 nodes, with a failure vs. a network partition:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d322e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/system-of-2.png" pf-restore-src="https://pdf.printfriendly.com/camo/82720236d76a90158480f396c9117503fad9973c/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d322e706e67" pf-data-width="739" pf-data-height="216" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="92" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">A system of 3 nodes, with a failure vs. a network partition:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d332e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/system-of-3.png" pf-restore-src="https://pdf.printfriendly.com/camo/b0ff7f5cb250d675448dd60a72198db0a5c9d4aa/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f73797374656d2d6f662d332e706e67" pf-data-width="710" pf-data-height="328" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="92" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">A system that enforces single-copy consistency must have some method to break symmetry: otherwise, it will split into two separate systems, which can diverge from each other and can no longer maintain the illusion of a single copy.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Network partition tolerance for systems that enforce single-copy consistency requires that during a network partition, only one partition of the system remains active since during a network partition it is not possible to prevent divergence (e.g. CAP theorem).</span></p>
<h3 id="majority-decisions" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Majority decisions</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This is why partition tolerant consensus algorithms rely on a majority vote. Requiring a majority of nodes - rather than all of the nodes (as in 2PC) - to agree on updates allows a minority of the nodes to be down, or slow, or unreachable due to a network partition. As long as </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="109.375" data-pf_rect_height="15" orig-style="null">(N/2 + 1)-of-N</code><span class="text-node"> nodes are up and accessible, the system can continue to operate.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Partition tolerant consensus algorithms use an odd number of nodes (e.g. 3, 5 or 7). With just two nodes, it is not possible to have a clear majority after a failure. For example, if the number of nodes is three, then the system is resilient to one node failure; with five nodes the system is resilient to two node failures.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">When a network partition occurs, the partitions behave asymmetrically. One partition will contain the majority of the nodes. Minority partitions will stop processing operations to prevent divergence during a network partition, but the majority partition can remain active. This ensures that only a single copy of the system state remains active.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Majorities are also useful because they can tolerate disagreement: if there is a perturbation or failure, the nodes may vote differently. However, since there can be only one majority decision, a temporary disagreement can at most block the protocol from proceeding (giving up liveness) but it cannot violate the single-copy consistency criterion (safety property).</span></p>
<h3 id="roles" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Roles</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">There are two ways one might structure a system: all nodes may have the same responsibilities, or nodes may have separate, distinct roles.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Consensus algorithms for replication generally opt for having distinct roles for each node. Having a single fixed leader or master server is an optimization that makes the system more efficient, since we know that all updates must pass through that server. Nodes that are not the leader just need to forward their requests to the leader.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Note that having distinct roles does not preclude the system from recovering from the failure of the leader (or any other role). Just because roles are fixed during normal operation doesn't mean that one cannot recover from failure by reassigning the roles after a failure (e.g. via a leader election phase). Nodes can reuse the result of a leader election until node failures and/or network partitions occur.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Both Paxos and Raft make use of distinct node roles. In particular, they have a leader node ("proposer" in Paxos) that is responsible for coordination during normal operation. During normal operation, the rest of the nodes are followers ("acceptors" or "voters" in Paxos).</span></p>
<h3 id="epochs" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Epochs</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Each period of normal operation in both Paxos and Raft is called an epoch ("term" in Raft). During each epoch only one node is the designated leader (a similar system is </span><a href="http://en.wikipedia.org/wiki/Japanese_era_name" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="371.265625" data-pf_rect_height="38" orig-style="null"><span class="text-node">used in Japan</span></a><span class="text-node"> where era names change upon imperial succession).</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f65706f63682e706e67" alt="replication" pf-orig-src="http://book.mixu.net/distsys/images/epoch.png" pf-restore-src="https://pdf.printfriendly.com/camo/86a1db22eb6f04943501e46fa82b9c064cb54f8d/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f65706f63682e706e67" pf-data-width="466" pf-data-height="171" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="92" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">After a successful election, the same leader coordinates until the end of the epoch. As shown in the diagram above (from the Raft paper), some elections may fail, causing the epoch to end immediately.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Epochs act as a logical clock, allowing other nodes to identify when an outdated node starts communicating - nodes that were partitioned or out of operation will have a smaller epoch number than the current one, and their commands are ignored.</span></p>
<h3 id="leader-changes-via-duels" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Leader changes via duels</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">During normal operation, a partition-tolerant consensus algorithm is rather simple. As we've seen earlier, if we didn't care about fault tolerance, we could just use 2PC. Most of the complexity really arises from ensuring that once a consensus decision has been made, it will not be lost and the protocol can handle leader changes as a result of a network or node failure.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">All nodes start as followers; one node is elected to be a leader at the start. During normal operation, the leader maintains a heartbeat which allows the followers to detect if the leader fails or becomes partitioned.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">When a node detects that a leader has become non-responsive (or, in the initial case, that no leader exists), it switches to an intermediate state (called "candidate" in Raft) where it increments the term/epoch value by one, initiates a leader election and competes to become the new leader.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">In order to be elected a leader, a node must receive a majority of the votes. One way to assign votes is to simply assign them on a first-come-first-served basis; this way, a leader will eventually be elected. Adding a random amount of waiting time between attempts at getting elected will reduce the number of nodes that are simultaneously attempting to get elected.</span></p>
<h3 id="numbered-proposals-within-an-epoch" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Numbered proposals within an epoch</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">During each epoch, the leader proposes one value at a time to be voted upon. Within each epoch, each proposal is numbered with a unique strictly increasing number. The followers (voters / acceptors) accept the first proposal they receive for a particular proposal number.</span></p>
<h3 id="normal-operation" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Normal operation</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">During normal operation, all proposals go through the leader node. When a client submits a proposal (e.g. an update operation), the leader contacts all nodes in the quorum. If no competing proposals exist (based on the responses from the followers), the leader proposes the value. If a majority of the followers accept the value, then the value is considered to be accepted.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Since it is possible that another node is also attempting to act as a leader, we need to ensure that once a single proposal has been accepted, its value can never change. Otherwise a proposal that has already been accepted might for example be reverted by a competing leader. Lamport states this as:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">P2: If a proposal with value </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node"> is chosen, then every higher-numbered proposal that is chosen has value </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node">.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">Ensuring that this property holds requires that both followers and proposers are constrained by the algorithm from ever changing a value that has been accepted by a majority. Note that "the value can never change" refers to the value of a single execution (or run / instance / decision) of the protocol. A typical replication algorithm will run multiple executions of the algorithm, but most discussions of the algorithm focus on a single run to keep things simple. We want to prevent the decision history from being altered or overwritten.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">In order to enforce this property, the proposers must first ask the followers for their (highest numbered) accepted proposal and value. If the proposer finds out that a proposal already exists, then it must simply complete this execution of the protocol, rather than making its own proposal. Lamport states this as:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">P2b. If a proposal with value </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node"> is chosen, then every higher-numbered proposal issued by any proposer has value </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node">.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">More specifically:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="209" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">P2c. For any </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">n</code><span class="text-node">, if a proposal with value </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node"> and number </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">n</code><span class="text-node"> is issued [by a leader], then there is a set </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">S</code><span class="text-node"> consisting of a majority of acceptors [followers] such that either (a) no acceptor in </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">S</code><span class="text-node"> has accepted any proposal numbered less than </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">n</code><span class="text-node">, or (b) </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">v</code><span class="text-node"> is the value of the highest-numbered proposal among all proposals numbered less than </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">n</code><span class="text-node"> accepted by the followers in </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">S</code><span class="text-node">.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This is the core of the Paxos algorithm, as well as algorithms derived from it. The value to be proposed is not chosen until the second phase of the protocol. Proposers must sometimes simply retransmit a previously made decision to ensure safety (e.g. clause b in P2c) until they reach a point where they know that they are free to impose their own proposal value (e.g. clause a).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">If multiple previous proposals exist, then the highest-numbered proposal value is proposed. Proposers may only attempt to impose their own value if there are no competing proposals at all.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">To ensure that no competing proposals emerge between the time the proposer asks each acceptor about its most recent value, the proposer asks the followers not to accept proposals with lower proposal numbers than the current one.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Putting the pieces together, reaching a decision using Paxos requires two rounds of communication:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="135" class="added-to-list1" orig-style="null">[ Proposer ] -&gt; Prepare(n)                                [ Followers ]
             &lt;- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -&gt; AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                &lt;- Accepted(n, value)</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">The prepare stage allows the proposer to learn of any competing or previous proposals. The second phase is where either a new value or a previously accepted value is proposed. In some cases - such as if two proposers are active at the same time (dueling); if messages are lost; or if a majority of the nodes have failed - then no proposal is accepted by a majority. But this is acceptable, since the decision rule for what value to propose converges towards a single value (the one with the highest proposal number in the previous attempt).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Indeed, according to the FLP impossibility result, this is the best we can do: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold. Paxos gives up liveness: it may have to delay decisions indefinitely until a point in time where there are no competing leaders, and a majority of nodes accept a proposal. This is preferable to violating the safety guarantees.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Of course, implementing this algorithm is much harder than it sounds. There are many small concerns which add up to a fairly significant amount of code even in the hands of experts. These are issues such as:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="494" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="133" orig-style="null">
<span class="text-node">practical optimizations:</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">avoiding repeated leader election via leadership leases (rather than heartbeats)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">avoiding repeated propose messages when in a stable state where the leader identity does not change</span></li>
</ul>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">ensuring that followers and proposers do not lose items in stable storage and that results stored in stable storage are not subtly corrupted (e.g. disk corruption)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null"><span class="text-node">enabling cluster membership to change in a safe manner (e.g. base Paxos depends on the fact that majorities always intersect in one node, which does not hold if the membership can change arbitrarily)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="76" orig-style="null"><span class="text-node">procedures for bringing a new replica up to date in a safe and efficient manner after a crash, disk loss or when a new node is provisioned</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null"><span class="text-node">procedures for snapshotting and garbage collecting the data required to guarantee safety after some reasonable period (e.g. balancing storage requirements and fault tolerance requirements)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Google's </span><a href="http://labs.google.com/papers/paxos_made_live.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="136.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos Made Live</span></a><span class="text-node"> paper details some of these challenges.</span></p>
<h2 id="partition-tolerant-consensus-algorithms-paxos-raft-zab" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="87" orig-style="null"><span class="text-node">Partition-tolerant consensus algorithms: Paxos, Raft, ZAB</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Hopefully, this has given you a sense of how a partition-tolerant consensus algorithm works. I encourage you to read one of the papers in the further reading section to get a grasp of the specifics of the different algorithms.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="46.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos</span></em><span class="text-node">. Paxos is one of the most important algorithms when writing strongly consistent partition tolerant replicated systems. It is used in many of Google's systems, including the </span><a href="http://research.google.com/archive/chubby.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="177.734375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Chubby lock manager</span></a><span class="text-node"> used by </span><a href="http://research.google.com/archive/bigtable.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.859375" data-pf_rect_height="19" orig-style="null"><span class="text-node">BigTable</span></a><span class="text-node">/</span><a href="http://research.google.com/pubs/pub36971.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="86.96875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Megastore</span></a><span class="text-node">, the Google File System as well as </span><a href="http://research.google.com/archive/spanner.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="68.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Spanner</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Paxos is named after the Greek island of Paxos, and was originally presented by Leslie Lamport in a paper called "The Part-Time Parliament" in 1998. It is often considered to be difficult to implement, and there have been a series of papers from companies with considerable distributed systems expertise explaining further practical details (see the further reading). You might want to read Lamport's commentary on this issue </span><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="36.875" data-pf_rect_height="19" orig-style="null"><span class="text-node">here</span></a><span class="text-node"> and </span><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#paxos-simple" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="36.875" data-pf_rect_height="19" orig-style="null"><span class="text-node">here</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">The issues mostly relate to the fact that Paxos is described in terms of a single round of consensus decision making, but an actual working implementation usually wants to run multiple rounds of consensus efficiently. This has led to the development of many </span><a href="http://en.wikipedia.org/wiki/Paxos_algorithm" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="380.84375" data-pf_rect_height="38" orig-style="null"><span class="text-node">extensions on the core protocol</span></a><span class="text-node"> that anyone interested in building a Paxos-based system still needs to digest. Furthermore, there are additional practical challenges such as how to facilitate cluster membership change.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="34.40625" data-pf_rect_height="19" orig-style="null"><span class="text-node">ZAB</span></em><span class="text-node">. ZAB - the Zookeeper Atomic Broadcast protocol is used in Apache Zookeeper. Zookeeper is a system which provides coordination primitives for distributed systems, and is used by many Hadoop-centric distributed systems for coordination (e.g. </span><a href="http://hbase.apache.org/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="52.90625" data-pf_rect_height="19" orig-style="null"><span class="text-node">HBase</span></a><span class="text-node">, </span><a href="http://storm-project.net/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="49.8125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Storm</span></a><span class="text-node">, </span><a href="http://kafka.apache.org/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="46.625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Kafka</span></a><span class="text-node">). Zookeeper is basically the open source community's version of Chubby. Technically speaking atomic broadcast is a problem different from pure consensus, but it still falls under the category of partition tolerant algorithms that ensure strong consistency.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="34.296875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Raft</span></em><span class="text-node">. Raft is a recent (2013) addition to this family of algorithms. It is designed to be easier to teach than Paxos, while providing the same guarantees. In particular, the different parts of the algorithm are more clearly separated and the paper also describes a mechanism for cluster membership change. It has recently seen adoption in </span><a href="https://github.com/coreos/etcd" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="35.078125" data-pf_rect_height="19" orig-style="null"><span class="text-node">etcd</span></a><span class="text-node"> inspired by ZooKeeper.</span></p>
<h2 id="replication-methods-with-strong-consistency" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Replication methods with strong consistency</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">In this chapter, we took a look at replication methods that enforce strong consistency. Starting with a contrast between synchronous work and asynchronous work, we worked our way up to algorithms that are tolerant of increasingly complex failures. Here are some of the key characteristics of each of the algorithms:</span></p>
<h4 id="primary-backup" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Primary/Backup</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Single, static master</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Replicated log, slaves are not involved in executing operations</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">No bounds on replication delay</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Not partition tolerant</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Manual/ad-hoc failover, not fault tolerant, "hot backup"</span></li>
</ul>
<h4 id="2pc" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">2PC</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Unanimous vote: commit or abort</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Static master</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">2PC cannot survive simultaneous failure of the coordinator and a node during a commit</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Not partition tolerant, tail latency sensitive</span></li>
</ul>
<h4 id="paxos" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Majority vote</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Dynamic master</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Robust to n/2-1 simultaneous failures as part of protocol</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">Less sensitive to tail latency</span></li>
</ul>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h2 id="further-reading" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Further reading</span></h2>
<h4 id="primary-backup-and-2pc" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Primary-backup and 2PC</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Replication+techniques+for+availability" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="307.9375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Replication techniques for availability</span></a><span class="text-node"> - Robbert van Renesse &amp; Rachid Guerraoui, 2010</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="306.4375" data-pf_rect_height="38" orig-style="null"><span class="text-node">Concurrency Control and Recovery in Database Systems</span></a></li>
</ul>
<h4 id="paxos" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="304" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="208.390625" data-pf_rect_height="19" orig-style="null"><span class="text-node">The Part-Time Parliament</span></a><span class="text-node"> - Leslie Lamport</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="158.234375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos Made Simple</span></a><span class="text-node"> - Leslie Lamport, 2001</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://research.google.com/archive/paxos_made_live.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="278.734375" data-pf_rect_height="38" orig-style="null"><span class="text-node">Paxos Made Live - An Engineering Perspective</span></a><span class="text-node"> - Chandra et al</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<a href="http://scholar.google.com/scholar?q=Paxos+Made+Practical" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="174.203125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Paxos Made Practical</span></a><span class="text-node"> - Mazieres, 2007</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://groups.csail.mit.edu/tds/paxos.html" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="250.03125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Revisiting the Paxos Algorithm</span></a><span class="text-node"> - Lynch et al</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="313.53125" data-pf_rect_height="38" orig-style="null"><span class="text-node">How to build a highly available system with consensus</span></a><span class="text-node"> - Butler Lampson</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="252.828125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Reconfiguring a State Machine</span></a><span class="text-node"> - Lamport et al - changing cluster membership</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="306.5625" data-pf_rect_height="57" orig-style="null"><span class="text-node">Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial</span></a><span class="text-node"> - Fred Schneider</span>
</li>
</ul>
<h4 id="raft-and-zab" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Raft and ZAB</span></h4>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="259.296875" data-pf_rect_height="38" orig-style="null"><span class="text-node">In Search of an Understandable Consensus Algorithm</span></a><span class="text-node">, Diego Ongaro, John Ousterhout, 2013</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><a href="http://www.youtube.com/watch?v=YbZ3zDzDnrw" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="208.625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Raft Lecture - User Study</span></a></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://labs.yahoo.com/publication/a-simple-totally-ordered-broadcast-protocol/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="281.09375" data-pf_rect_height="38" orig-style="null"><span class="text-node">A simple totally ordered broadcast protocol</span></a><span class="text-node"> - Junqueira, Reed, 2008</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<a href="http://labs.yahoo.com/publication/zab-high-performance-broadcast-for-primary-backup-systems/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="236.75" data-pf_rect_height="19" orig-style="null"><span class="text-node">ZooKeeper Atomic Broadcast</span></a><span class="text-node"> - Reed, 2011</span>
</li>
</ul>

<h1 id="-chapter_number-replication-weak-consistency-model-protocols" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null"><span class="text-node">5. Replication: weak consistency model protocols</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Now that we've taken a look at protocols that can enforce single-copy consistency under an increasingly realistic set of supported failure cases, let's turn our attention at the world of options that opens up once we let go of the requirement of single-copy consistency.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">By and large, it is hard to come up with a single dimension that defines or characterizes the protocols that allow for replicas to diverge. Most such protocols are highly available, and the key issue is more whether or not the end users find the guarantees, abstractions and APIs useful for their purpose in spite of the fact that the replicas may diverge when node and/or network failures occur.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Why haven't weakly consistent systems been more popular?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">As I stated in the introduction, I think that much of distributed programming is about dealing with the implications of two consequences of distribution:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">that information travels at the speed of light</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">that independent things fail independently</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The implication that follows from the limitation on the speed at which information travels is that nodes experience the world in different, unique ways. Computation on a single node is easy, because everything happens in a predictable global total order. Computation on a distributed system is difficult, because there is no global total order.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">For the longest while (e.g. decades of research), we've solved this problem by introducing a global total order. I've discussed the many methods for achieving strong consistency by creating order (in a fault-tolerant manner) where there is no naturally occurring total order.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Of course, the problem is that enforcing order is expensive. This breaks down in particular with large scale internet systems, where a system needs to remain available. A system enforcing strong consistency doesn't behave like a distributed system: it behaves like a single system, which is bad for availability during a partition.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Furthermore, for each operation, often a majority of the nodes must be contacted - and often not just once, but twice (as you saw in the discussion on 2PC). This is particularly painful in systems that need to be geographically distributed to provide adequate performance for a global user base.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">So behaving like a single system by default is perhaps not desirable.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Perhaps what we want is a system where we can write code that doesn't use expensive coordination, and yet returns a "usable" value. Instead of having a single truth, we will allow different replicas to diverge from each other - both to keep things efficient but also to tolerate partitions - and then try to find a way to deal with the divergence in some manner.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Eventual consistency expresses this idea: that nodes can for some time diverge from each other, but that eventually they will agree on the value.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Within the set of systems providing eventual consistency, there are two types of system designs:</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="318.65625" data-pf_rect_height="38" orig-style="null"><span class="text-node">Eventual consistency with probabilistic guarantees</span></em><span class="text-node">. This type of system can detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution. In other words, conflicting updates will sometimes result in overwriting a newer value with an older one and some anomalies can be expected to occur during normal operation (or during partitions).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">In recent years, the most influential system design offering single-copy consistency is Amazon's Dynamo, which I will discuss as an example of a system that offers eventual consistency with probabilistic guarantees.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="365.734375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Eventual consistency with strong guarantees</span></em><span class="text-node">. This type of system guarantees that the results converge to a common value equivalent to some correct sequential execution. In other words, such systems do not produce any anomalous results; without any coordination you can build replicas of the same service, and those replicas can communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">CRDT's (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering. They are provably convergent, but the data types that can be implemented as CRDT's are limited.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">The CALM (consistency as logical monotonicity) conjecture is an alternative expression of the same principle: it equates logical monotonicity with convergence. If we can conclude that something is logically monotonic, then it is also safe to run without coordination. Confluence analysis - in particular, as applied for the Bloom programming language - can be used to guide programmer decisions about when and where to use the coordination techniques from strongly consistent systems and when it is safe to execute without coordination.</span></p>
<h2 id="reconciling-different-operation-orders" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Reconciling different operation orders</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">What does a system that does not enforce single-copy consistency look like?  Let's try to make this more concrete by looking at a few examples.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Perhaps the most obvious characteristic of systems that do not enforce single-copy consistency is that they allow replicas to diverge from each other. This means that there is no strictly defined pattern of communication: replicas can be separated from each other and yet continue to be available and accept writes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Let's imagine a system of three replicas, each of which is partitioned from the others. For example, the replicas might be in different datacenters and for some reason unable to communicate. Each replica remains available during the partition, accepting both reads and writes from some set of clients:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="135" class="added-to-list1" orig-style="null">[Clients]   - &gt; [A]

--- Partition ---

[Clients]   - &gt; [B]

--- Partition ---

[Clients]   - &gt; [C]</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">After some time, the partitions heal and the replica servers exchange information. They have received different updates from different clients and have diverged each other, so some sort of reconciliation needs to take place. What we would like to happen is that all of the replicas converge to the same result.</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="75" class="added-to-list1" orig-style="null">[A] \
    --&gt; [merge]
[B] /     |
          |
[C] ----[merge]---&gt; result</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Another way to think about systems with weak consistency guarantees is to imagine a set of clients sending messages to two replicas in some order. Because there is no coordination protocol that enforces a single total order, the messages can get delivered in different orders at the two replicas:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">[Clients]  --&gt; [A]  1, 2, 3
[Clients]  --&gt; [B]  2, 3, 1</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">This is, in essence, the reason why we need coordination protocols. For example, assume that we are trying to concatenate a string and the operations in messages 1, 2 and 3 are:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="45" class="added-to-list1" orig-style="null">1: { operation: concat('Hello ') }
2: { operation: concat('World') }
3: { operation: concat('!') }</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Then, without coordination, A will produce "Hello World!", and B will produce "World!Hello ".</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">A: concat(concat(concat('', 'Hello '), 'World'), '!') = 'Hello World!'
B: concat(concat(concat('', 'World'), '!'), 'Hello ') = 'World!Hello '</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">This is, of course, incorrect. Again, what we'd like to happen is that the replicas converge to the same result.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Keeping these two examples in mind, let's look at Amazon's Dynamo first to establish a baseline, and then discuss a number of novel approaches to building systems with weak consistency guarantees, such as CRDT's and the CALM theorem.</span></p>
<h2 id="amazon-s-dynamo" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Amazon's Dynamo</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Amazon's Dynamo system design (2007) is probably the best-known system that offers weak consistency guarantees but high availability. It is the basis for many other real world systems, including LinkedIn's Voldemort, Facebook's Cassandra and Basho's Riak.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Dynamo is an eventually consistent, highly available key-value store. A key value store is like a large hash table: a client can set values via </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="117.1875" data-pf_rect_height="15" orig-style="null">set(key, value)</code><span class="text-node"> and retrieve them by key using </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="62.5" data-pf_rect_height="15" orig-style="null">get(key)</code><span class="text-node">. A Dynamo cluster consists of N peer nodes; each node has a set of keys which is it responsible for storing.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Dynamo prioritizes availability over consistency; it does not guarantee single-copy consistency. Instead, replicas may diverge from each other when values are written; when a key is read, there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">For many features on Amazon, it is more important to avoid outages than it is to ensure that data is perfectly consistent, as an outage can lead to lost business and a loss of credibility. Furthermore, if the data is not particularly important, then a weakly consistent system can provide better performance and higher availability at a lower cost than a traditional RDBMS.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Since Dynamo is a complete system design, there are many different parts to look at beyond the core replication task. The diagram below illustrates some of the tasks; notably, how a write is routed to a node and written to multiple replicas.</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="255" class="added-to-list1" orig-style="null">[ Client ]
    |
( Mapping keys to nodes )
    |
    V
[ Node A ]
    |     \
( Synchronous replication task: minimum durability )
    |        \
[ Node B]  [ Node C ]
    A
    |
( Conflict detection; asynchronous replication task:
  ensuring that partitioned / recovered nodes recover )
    |
    V
[ Node D]</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">After looking at how a write is initially accepted, we'll look at how conflicts are detected, as well as the asynchronous replica synchronization task. This task is needed because of the high availability design, in which nodes may be temporarily unavailable (down or partitioned). The replica synchronization task ensures that nodes can catch up fairly rapidly even after a failure.</span></p>
<h3 id="consistent-hashing" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Consistent hashing</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Whether we are reading or writing, the first thing that needs to happen is that we need to locate where the data should live on the system. This requires some type of key-to-node mapping.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">In Dynamo, keys are mapped to nodes using a hashing technique known as </span><a href="https://github.com/mixu/vnodehash" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="319.15625" data-pf_rect_height="38" orig-style="null"><span class="text-node">consistent hashing</span></a><span class="text-node"> (which I will not discuss in detail). The main idea is that a key can be mapped to a set of nodes responsible for it by a simple calculation on the client. This means that a client can locate keys without having to query the system for the location of each key; this saves system resources as hashing is generally faster than performing a remote procedure call.</span></p>
<h3 id="partial-quorums" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Partial quorums</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Once we know where a key should be stored, we need to do some work to persist the value. This is a synchronous task; the reason why we will immediately write the value onto multiple nodes is to provide a higher level of durability (e.g. protection from the immediate failure of a node).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Just like Paxos or Raft, Dynamo uses quorums for replication. However, Dynamo's quorums are sloppy (partial) quorums rather than strict (majority) quorums.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Informally, a strict quorum system is a quorum system with the property that any two quorums (sets) in the quorum system overlap. Requiring a majority to vote for an update before accepting it guarantees that only a single history is admitted since each majority quorum must overlap in at least one node. This was the property that Paxos, for example, relied on.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Partial quorums do not have that property; what this means is that a majority is not required and that different subsets of the quorum may contain different versions of the same data. The user can choose the number of nodes to write to and read from:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null"><span class="text-node">the user can choose some number W-of-N nodes required for a write to succeed; and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">the user can specify the number of nodes (R-of-N) to be contacted during a read.</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">W</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">R</code><span class="text-node"> specify the number of nodes that need to be involved to a write or a read. Writing to more nodes makes writes slightly slower but increases the probability that the value is not lost; reading from more nodes increases the probability that the value read is up to date.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">The usual recommendation is that </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">R + W &gt; N</code><span class="text-node">, because this means that the read and write quorums overlap in one node - making it less likely that a stale value is returned. A typical configuration is </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">N = 3</code><span class="text-node"> (e.g. a total of three replicas for each value); this means that the user can choose between:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="45" class="added-to-list1" orig-style="null"> R = 1, W = 3;
 R = 2, W = 2 or
 R = 3, W = 1</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">More generally, again assuming </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">R + W &gt; N</code><span class="text-node">:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">R = 1</code><span class="text-node">, </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">W = N</code><span class="text-node">: fast reads, slow writes</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">R = N</code><span class="text-node">, </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">W = 1</code><span class="text-node">: fast writes, slow reads</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null">
<code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="54.6875" data-pf_rect_height="15" orig-style="null">R = N/2</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="85.9375" data-pf_rect_height="15" orig-style="null">W = N/2 + 1</code><span class="text-node">: favorable to both</span>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">N is rarely more than 3, because keeping that many copies of large amounts of data around gets expensive!</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">As I mentioned earlier, the Dynamo paper has inspired many other similar designs. They all use the same partial quorum based replication approach, but with different defaults for N, W and R:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Basho's Riak (N = 3, R = 2, W = 2 default)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Linkedin's Voldemort (N = 2 or 3, R = 1, W = 1 default)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Apache's Cassandra (N = 3, R = 1, W = 1 default)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">There is another detail: when sending a read or write request, are all N nodes asked to respond (Riak), or only a number of nodes that meets the minimum (e.g. R or W; Voldemort). The "send-to-all" approach is faster and less sensitive to latency (since it only waits for the fastest R or W nodes of N) but also less efficient, while the "send-to-minimum" approach is more sensitive to latency (since latency communicating with a single node will delay the operation) but also more efficient (fewer messages / connections overall).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">What happens when the read and write quorums overlap, e.g. (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">R + W &gt; N</code><span class="text-node">)? Specifically, it is often claimed that this results in "strong consistency".</span></p>
<h3 id="is-r-w-n-the-same-as-strong-consistency-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Is R + W &gt; N the same as "strong consistency"?</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">No.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">It's not completely off base: a system where </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="369.59375" data-pf_rect_height="34" orig-style="null">R + W &gt; N</code><span class="text-node"> can detect read/write conflicts, since any read quorum and any write quorum share a member. E.g. at least one node is in both quorums:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">   1     2   N/2+1     N/2+2    N
  [...] [R]  [R + W]   [W]    [...]</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">This guarantees that a previous write will be seen by a subsequent read. However, this only holds if the nodes in N never change. Hence, Dynamo doesn't qualify, because in Dynamo the cluster membership can change if nodes fail.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="247" class="added-to-list1" orig-style="null"><span class="text-node">Dynamo is designed to be always writable. It has a mechanism which handles node failures by adding a different, unrelated server into the set of nodes responsible for certain keys when the original server is down. This means that the quorums are no longer guaranteed to always overlap. Even </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">R = W = N</code><span class="text-node"> would not qualify, since while the quorum sizes are equal to N, the nodes in those quorums can change during a failure. Concretely, during a partition, if a sufficient number of nodes cannot be reached, Dynamo will add new nodes to the quorum from unrelated but accessible nodes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Furthermore, Dynamo doesn't handle partitions in the manner that a system enforcing a strong consistency model would: namely, writes are allowed on both sides of a partition, which means that for at least some time the system does not act as a single copy. So calling </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">R + W &gt; N</code><span class="text-node"> "strongly consistent" is misleading; the guarantee is merely probabilistic - which is not what strong consistency refers to.</span></p>
<h3 id="conflict-detection-and-read-repair" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Conflict detection and read repair</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Systems that allow replicas to diverge must have a way to eventually reconcile two different values. As briefly mentioned during the partial quorum approach, one way to do this is to detect conflicts at read time, and then apply some conflict resolution method. But how is this done?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">In general, this is done by tracking the causal history of a piece of data by supplementing it with some metadata. Clients must keep the metadata information when they read data from the system, and must return back the metadata value when writing to the database.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">We've already encountered a method for doing this: vector clocks can be used to represent the history of a value. Indeed, this is what the original Dynamo design uses for detecting conflicts.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">However, using vector clocks is not the only alternative. If you look at many practical system designs, you can deduce quite a bit about how they work by looking at the metadata that they track.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="105.015625" data-pf_rect_height="19" orig-style="null"><span class="text-node">No metadata</span></em><span class="text-node">. When a system does not track metadata, and only returns the value (e.g. via a client API), it cannot really do anything special about concurrent writes. A common rule is that the last writer wins: in other words, if two writers are writing at the same time, only the value from the slowest writer is kept around.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="98.1875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Timestamps</span></em><span class="text-node">. Nominally, the value with the higher timestamp value wins. However, if time is not carefully synchronized, many odd things can happen where old data from a system with a faulty or fast clock overwrites newer values. Facebook's Cassandra is a Dynamo variant that uses timestamps instead of vector clocks.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="136.8125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Version numbers</span></em><span class="text-node">. Version numbers may avoid some of the issues related with using timestamps. Note that the smallest mechanism that can accurately track causality when multiple histories are possible are vector clocks, not version numbers.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="107.796875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Vector clocks</span></em><span class="text-node">. Using vector clocks, concurrent and out of date updates can be detected. Performing read repair then becomes possible, though in some cases (concurrent changes) we need to ask the client to pick a value. This is because if the changes are concurrent and we know nothing more about the data (as is the case with a simple key-value store), then it is better to ask than to discard data arbitrarily.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">When reading a value, the client contacts </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">R</code><span class="text-node"> of </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">N</code><span class="text-node"> nodes and asks them for the latest value for a key. It takes all the responses, discards the values that are strictly older (using the vector clock value to detect this). If there is only one unique vector clock + value pair, it returns that. If there are multiple vector clock + value pairs that have been edited concurrently (e.g. are not comparable), then all of those values are returned.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">As is obvious from the above, read repair may return multiple values. This means that the client / application developer must occasionally handle these cases by picking a value based on some use-case specific criterion.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">In addition, a key component of a practical vector clock system is that the clocks cannot be allowed to grow forever - so there needs to be a procedure for occasionally garbage collecting the clocks in a safe manner to balance fault tolerance with storage requirements.</span></p>
<h3 id="replica-synchronization-gossip-and-merkle-trees" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="44" orig-style="null"><span class="text-node">Replica synchronization: gossip and Merkle trees</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Given that the Dynamo system design is tolerant of node failures and network partitions, it needs a way to deal with nodes rejoining the cluster after being partitioned, or when a failed node is replaced or partially recovered.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Replica synchronization is used to bring nodes up to date after a failure, and for periodically synchronizing replicas with each other.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Gossip is a probabilistic technique for synchronizing replicas. The pattern of communication (e.g. which node contacts which node) is not determined in advance. Instead, nodes have some probability </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">p</code><span class="text-node"> of attempting to synchronize with each other. Every </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">t</code><span class="text-node"> seconds, each node picks a node to communicate with. This provides an additional mechanism beyond the synchronous task (e.g. the partial quorum writes) which brings the replicas up to date.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Gossip is scalable, and has no single point of failure, but can only provide probabilistic guarantees.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">In order to make the information exchange during replica synchronization efficient, Dynamo uses a technique called Merkle trees, which I will not cover in detail. The key idea is that a data store can be hashed at multiple different levels of granularity: a hash representing the whole content, half the keys, a quarter of the keys and so on.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">By maintaining this fairly granular hashing, nodes can compare their data store content much more efficiently than a naive technique. Once the nodes have identified which keys have different values, they exchange the necessary information to bring the replicas up to date.</span></p>
<h3 id="dynamo-in-practice-probabilistically-bounded-staleness-pbs-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="66" orig-style="null"><span class="text-node">Dynamo in practice: probabilistically bounded staleness (PBS)</span></h3>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">And that pretty much covers the Dynamo system design:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">consistent hashing to determine key placement</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">partial quorums for reading and writing</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">conflict detection and read repair via vector clocks and</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="19" orig-style="null"><span class="text-node">gossip for replica synchronization</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">How might we characterize the behavior of such a system? A fairly recent paper from Bailis et al. (2012) describes an approach called </span><a href="http://pbs.cs.berkeley.edu/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="33.46875" data-pf_rect_height="19" orig-style="null"><span class="text-node">PBS</span></a><span class="text-node"> (probabilistically bounded staleness) uses simulation and data collected from a real world system to characterize the expected behavior of such a system.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">PBS estimates the degree of inconsistency by using information about the anti-entropy (gossip) rate, the network latency and local processing delay to estimate the expected level of consistency of reads. It has been implemented in Cassandra, where timing information is piggybacked on other messages and an estimate is calculated based on a sample of this information in a Monte Carlo simulation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Based on the paper, during normal operation eventually consistent data stores are often faster and can read a consistent state within tens or hundreds of milliseconds. The table below illustrates amount of time required from a 99.9% probability of consistent reads given different </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">R</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">W</code><span class="text-node"> settings on empirical timing data from LinkedIn (SSD and 15k RPM disks) and Yammer:</span></p>
<p class="img-container added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="27" orig-style="null"><img src="./Distributed systems_files/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7062732e706e67" alt="from the PBS paper" pf-orig-src="http://book.mixu.net/distsys/images/pbs.png" pf-restore-src="https://pdf.printfriendly.com/camo/f2b98b0980aceb4c15ac7580c165c7113af4d83f/687474703a2f2f626f6f6b2e6d6978752e6e65742f646973747379732f696d616765732f7062732e706e67" pf-data-width="1022" pf-data-height="245" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="165" data-pf_rect_height="23" class="pf-large-image flex-width pf-size-full blockImage" orig-style="null" srcset=""></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">For example, going from </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">R=1</code><span class="text-node">, </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">W=1</code><span class="text-node"> to </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">R=2</code><span class="text-node">, </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">W=1</code><span class="text-node"> in the Yammer case reduces the inconsistency window from 1352 ms to 202 ms - while keeping the read latencies lower (32.6 ms) than the fastest strict quorum (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">R=3</code><span class="text-node">, </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">W=1</code><span class="text-node">; 219.27 ms).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">For more details, have a look at the </span><a href="http://pbs.cs.berkeley.edu/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="326.6875" data-pf_rect_height="38" orig-style="null"><span class="text-node">PBS website</span></a><span class="text-node">  and the associated paper.</span></p>
<h2 id="disorderly-programming" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Disorderly programming</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Let's look back at the examples of the kinds of situations that we'd like to resolve. The first scenario consisted of three different servers behind partitions; after the partitions healed, we wanted the servers to converge to the same value. Amazon's Dynamo made this possible by reading from </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">R</code><span class="text-node"> out of </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">N</code><span class="text-node"> nodes and then performing read reconciliation.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">In the second example, we considered a more specific operation: string concatenation. It turns out that there is no known technique for making string concatenation resolve to the same value without imposing an order on the operations (e.g. without expensive coordination). However, there are operations which can be applied safely in any order, where a simple register would not be able to do so. As Pat Helland wrote:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="114" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">... operation-centric work can be made commutative (with the right operations and the right semantics) where a simple READ/WRITE semantic does not lend itself to commutativity.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">For example, consider a system that implements a simple accounting system with the </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">debit</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="46.875" data-pf_rect_height="15" orig-style="null">credit</code><span class="text-node"> operations in two different ways:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<span class="text-node">using a register with </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="31.25" data-pf_rect_height="15" orig-style="null">read</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">write</code><span class="text-node"> operations, and</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<span class="text-node">using a integer data type with native </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">debit</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="46.875" data-pf_rect_height="15" orig-style="null">credit</code><span class="text-node"> operations</span>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">The latter implementation knows more about the internals of the data type, and so it can preserve the intent of the operations in spite of the operations being reordered. Debiting or crediting can be applied in any order, and the end result is the same:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">100 + credit(10) + credit(20) = 130 and
100 + credit(20) + credit(10) = 130</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node"> However, writing a fixed value cannot be done in any order: if writes are reordered, the one of the writes will overwrite the other:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">100 + write(110) + write(130) = 130 but
100 + write(130) + write(110) = 110</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Let's take the example from the beginning of this chapter, but use a different operation. In this scenario, clients are sending messages to two nodes, which see the operations in different orders:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">[Clients]  --&gt; [A]  1, 2, 3
[Clients]  --&gt; [B]  2, 3, 1</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">Instead of string concatenation, assume that we are looking to find the largest value (e.g. MAX()) for a set of integers. The messages 1, 2 and 3 are:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="45" class="added-to-list1" orig-style="null">1: { operation: max(previous, 3) }
2: { operation: max(previous, 5) }
3: { operation: max(previous, 7) }</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Then, without coordination, both A and B will converge to 7, e.g.:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="30" class="added-to-list1" orig-style="null">A: max(max(max(0, 3), 5), 7) = 7
B: max(max(max(0, 5), 7), 3) = 7</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">In both cases, two replicas see updates in different order, but we are able to merge the results in a way that has the same result in spite of what the order is. The result converges to the same answer in both cases because of the merge procedure (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="23.4375" data-pf_rect_height="15" orig-style="null">max</code><span class="text-node">) we used.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">It is likely not possible to write a merge procedure that works for all data types. In Dynamo, a value is a binary blob, so the best that can be done is to expose it and ask the application to handle each conflict.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">However, if we know that the data is of a more specific type, handling these kinds of conflicts becomes possible. CRDT's are data structures designed to provide data types that will always converge, as long as they see the same set of operations (in any order).</span></p>
<h2 id="crdts-convergent-replicated-data-types" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">CRDTs: Convergent replicated data types</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">CRDTs (convergent replicated datatypes) exploit knowledge regarding the commutativity and associativity of specific operations on specific datatypes.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">In order for a set of operations to converge on the same value in an environment where replicas only communicate occasionally, the operations need to be order-independent and insensitive to (message) duplication/redelivery. Thus, their operations need to be:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<span class="text-node">Associative (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="117.1875" data-pf_rect_height="15" orig-style="null">a+(b+c)=(a+b)+c</code><span class="text-node">), so that grouping doesn't matter</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<span class="text-node">Commutative (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="54.6875" data-pf_rect_height="15" orig-style="null">a+b=b+a</code><span class="text-node">), so that order of application doesn't matter</span>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null">
<span class="text-node">Idempotent (</span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">a+a=a</code><span class="text-node">), so that duplication does not matter</span>
</li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">It turns out that these structures are already known in mathematics; they are known as join or meet </span><a href="http://en.wikipedia.org/wiki/Semilattice" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="97.171875" data-pf_rect_height="19" orig-style="null"><span class="text-node">semilattices</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">A </span><a href="http://en.wikipedia.org/wiki/Lattice_%28order%29" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="51.015625" data-pf_rect_height="19" orig-style="null"><span class="text-node">lattice</span></a><span class="text-node"> is a partially ordered set with a distinct top (least upper bound) and a distinct bottom (greatest lower bound). A semilattice is like a lattice, but one that only has a distinct top or bottom. A join semilattice is one with a distinct top (least upper bound) and a meet semilattice is one with a distinct bottom (greatest lower bound).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Any data type that be expressed as a semilattice can be implemented as a data structure which guarantees convergence. For example, calculating the </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">max()</code><span class="text-node"> of a set of values will always return the same result regardless of the order in which the values were received, as long as all values are eventually received, because the </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="39.0625" data-pf_rect_height="15" orig-style="null">max()</code><span class="text-node"> operation is associative, commutative and idempotent.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">For example, here are two lattices: one drawn for a set, where the merge operator is </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="93.75" data-pf_rect_height="15" orig-style="null">union(items)</code><span class="text-node"> and one drawn for a strictly increasing integer counter, where the merge operator is </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="85.9375" data-pf_rect_height="15" orig-style="null">max(values)</code><span class="text-node">:</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="75" class="added-to-list1" orig-style="null">   { a, b, c }              7
  /      |    \            /  \
{a, b} {b,c} {a,c}        5    7
  |  \  /  | /           /   |  \
  {a} {b} {c}            3   5   7</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">With data types that can be expressed as semilattices, you can have replicas communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information. That is a powerful property that can be guaranteed as long as the prerequisites hold.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="285" class="added-to-list1" orig-style="null"><span class="text-node">However, expressing a data type as a semilattice often requires some level of interpretation. Many data types have operations which are not in fact order-independent. For example, adding items to a set is associative, commutative and idempotent. However, if we also allow items to be removed from a set, then we need some way to resolve conflicting operations, such as </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="46.875" data-pf_rect_height="15" orig-style="null">add(A)</code><span class="text-node"> and </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="70.3125" data-pf_rect_height="15" orig-style="null">remove(A)</code><span class="text-node">. What does it mean to remove an element if the local replica never added it? This resolution has to be specified in a manner that is order-independent, and there are several different choices with different tradeoffs.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This means that several familiar data types have more specialized implementations as CRDT's which make a different tradeoff in order to resolve conflicts in an order-independent manner. Unlike a key-value store which simply deals with registers (e.g. values that are opaque blobs from the perspective of the system), someone using CRDTs must use the right data type to avoid anomalies.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Some examples of the different data types specified as CRDT's include:</span></p>
<ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="589" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="152" orig-style="null">
<span class="text-node">Counters</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="133" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">Grow-only counter (merge = max(values); payload = single integer)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" orig-style="null"><span class="text-node">Positive-negative counter (consists of two grow counters, one for increments and another for decrements)</span></li>
</ul>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="114" orig-style="null">
<span class="text-node">Registers</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="95" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">Last Write Wins -register (timestamps or version numbers; merge = max(ts); payload = blob)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null"><span class="text-node">Multi-valued -register (vector clocks; merge = take both)</span></li>
</ul>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="285" orig-style="null">
<span class="text-node">Sets</span><ul class="list added-to-list1" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="266" orig-style="null">
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="57" orig-style="null"><span class="text-node">Grow-only set (merge = union(items); payload = set; no removal)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="76" orig-style="null"><span class="text-node">Two-phase set (consists of two sets, one for adding, and another for removing; elements can be added once and removed once)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null"><span class="text-node">Unique set (an optimized version of the two-phase set)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null"><span class="text-node">Last write wins set (merge = max(ts); payload = set)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="38" orig-style="null"><span class="text-node">Positive-negative set (consists of one PN-counter per set item)</span></li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="19" orig-style="null"><span class="text-node">Observed-remove set</span></li>
</ul>
</li>
<li data-pf_style_display="list-item" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="38" orig-style="null"><span class="text-node">Graphs and text sequences (see the paper)</span></li>
</ul>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">To ensure anomaly-free operation, you need to find the right data type for your specific application - for example, if you know that you will only remove an item once, then a two-phase set works; if you will only ever add items to a set and never remove them, then a grow-only set works.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Not all data structures have known implementations as CRDTs, but there are CRDT implementations for booleans, counters, sets, registers and graphs in the recent (2011) </span><a href="http://hal.inria.fr/docs/00/55/55/88/PDF/techreport.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="258.453125" data-pf_rect_height="19" orig-style="null"><span class="text-node">survey paper from Shapiro et al</span></a><span class="text-node">.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Interestingly, the register implementations correspond directly with the implementations that key value stores use: a last-write-wins register uses timestamps or some equivalent and simply converges to the largest timestamp value; a multi-valued register corresponds to the Dynamo strategy of retaining, exposing and reconciling concurrent changes. For the details, I recommend that you take a look at the papers in the further reading section of this chapter.</span></p>
<h2 id="the-calm-theorem" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">The CALM theorem</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">The CRDT data structures were based on the recognition that data structures expressible as semilattices are convergent. But programming is about more than just evolving state, unless you are just implementing a data store.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Clearly, order-independence is an important property of any computation that converges: if the order in which data items are received influences the result of the computation, then there is no way to execute a computation without guaranteeing order.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">However, there are many programming models in which the order of statements does not play a significant role. For example, in the </span><a href="http://en.wikipedia.org/wiki/MapReduce" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="151.296875" data-pf_rect_height="19" orig-style="null"><span class="text-node">MapReduce model</span></a><span class="text-node">, both the Map and the Reduce tasks are specified as stateless tuple-processing tasks that need to be run on a dataset. Concrete decisions about how and in what order data is routed to the tasks is not specified explicitly, instead, the batch job scheduler is responsible for scheduling the tasks to run on the cluster.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Similarly, in SQL one specifies the query, but not how the query is executed. The query is simply a declarative description of the task, and it is the job of the query optimizer to figure out an efficient way to execute the query (across multiple machines, databases and tables).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Of course, these programming models are not as permissive as a general purpose programming language. MapReduce tasks need to be expressible as stateless tasks in an acyclic dataflow program; SQL statements can execute fairly sophisticated computations but many things are hard to express in it.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="266" class="added-to-list1" orig-style="null"><span class="text-node">However, it should be clear from these two examples that there are many kinds of data processing tasks which are amenable to being expressed in a declarative language where the order of execution is not explicitly specified. Programming models which express a desired result while leaving the exact order of statements up to an optimizer to decide often have semantics that are order-independent. This means that such programs may be possible to execute without coordination, since they depend on the inputs they receive but not necessarily the specific order in which the inputs are received.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">The key point is that such programs </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="58.515625" data-pf_rect_height="19" orig-style="null"><span class="text-node">may be</span></em><span class="text-node"> safe to execute without coordination. Without a clear rule that characterizes what is safe to execute without coordination, and what is not, we cannot implement a program while remaining certain that the result is correct.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">This is what the CALM theorem is about. The CALM theorem is based on a recognition of the link between logical monotonicity and useful forms of eventual consistency (e.g. confluence / convergence). It states that logically monotonic programs are guaranteed to be eventually consistent.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" class="added-to-list1" orig-style="null"><span class="text-node">Then, if we know that some computation is logically monotonic, then we know that it is also safe to execute without coordination.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">To better understand this, we need to contrast monotonic logic (or monotonic computations) with </span><a href="http://plato.stanford.edu/entries/logic-nonmonotonic/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="164.859375" data-pf_rect_height="19" orig-style="null"><span class="text-node">non-monotonic logic</span></a><span class="text-node"> (or non-monotonic computations).</span></p>
<dl data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null">
  <dt data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Monotony</span></dt>
  <dd data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="344" data-pf_rect_height="57" orig-style="null">
<span class="text-node">if sentence </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">φ</code><span class="text-node"> is a consequence of a set of premises </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">Γ</code><span class="text-node">, then it can also be inferred from any set </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">Δ</code><span class="text-node"> of premises extending </span><code data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="7.8125" data-pf_rect_height="15" orig-style="null">Γ</code>
</dd>
</dl>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Most standard logical frameworks are monotonic: any inferences made within a framework such as first-order logic, once deductively valid, cannot be invalidated by new information. A non-monotonic logic is a system in which that property does not hold - in other words, if some conclusions can be invalidated by learning new knowledge.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">Within the artificial intelligence community, non-monotonic logics are associated with </span><a href="http://plato.stanford.edu/entries/reasoning-defeasible/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="168.234375" data-pf_rect_height="19" orig-style="null"><span class="text-node">defeasible reasoning</span></a><span class="text-node"> - reasoning, in which assertions made utilizing partial information can be invalidated by new knowledge. For example, if we learn that Tweety is a bird, we'll assume that Tweety can fly; but if we later learn that Tweety is a penguin, then we'll have to revise our conclusion.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">Monotonicity concerns the relationship between premises (or facts about the world) and conclusions (or assertions about the world). Within a monotonic logic, we know that our results are retraction-free: </span><a href="http://en.wikipedia.org/wiki/Monotonicity_of_entailment" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="80.53125" data-pf_rect_height="19" orig-style="null"><span class="text-node">monotone</span></a><span class="text-node"> computations do not need to be recomputed or coordinated; the answer gets more accurate over time. Once we know that Tweety is a bird (and that we're reasoning using monotonic logic), we can safely conclude that Tweety can fly and that nothing we learn can invalidate that conclusion.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">While any computation that produces a human-facing result can be interpreted as an assertion about the world (e.g. the value of "foo" is "bar"), it is difficult to determine whether a computation in a von Neumann machine based programming model is monotonic, because it is not exactly clear what the relationship between facts and assertions are and whether those relationships are monotonic.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">However, there are a number of programming models for which determining monotonicity is possible. In particular, </span><a href="http://en.wikipedia.org/wiki/Relational_algebra" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="144.671875" data-pf_rect_height="19" orig-style="null"><span class="text-node">relational algebra</span></a><span class="text-node"> (e.g. the theoretical underpinnings of SQL) and </span><a href="http://en.wikipedia.org/wiki/Datalog" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="63.28125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Datalog</span></a><span class="text-node"> provide highly expressive languages that have well-understood interpretations.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Both basic Datalog and relational algebra (even with recursion) are known to be monotonic. More specifically, computations expressed using a certain set of basic operators are known to be monotonic (selection, projection, natural join, cross product, union and recursive Datalog without negation), and non-monotonicity is introduced by using more advanced operators (negation, set difference, division, universal quantification, aggregation).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This means that computations expressed using a significant number of operators (e.g. map, filter, join, union, intersection) in those systems are logically monotonic; any computations using those operators are also monotonic and thus safe to run without coordination. Expressions that make use of negation and aggregation, on the other hand, are not safe to run without coordination.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">It is important to realize the connection between non-monotonicity and operations that are expensive to perform in a distributed system. Specifically, both </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="299.203125" data-pf_rect_height="38" orig-style="null"><span class="text-node">distributed aggregation</span></em><span class="text-node"> and </span><em data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="182.984375" data-pf_rect_height="19" orig-style="null"><span class="text-node">coordination protocols</span></em><span class="text-node"> can be considered to be a form of negation. As Joe Hellerstein </span><a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-90.pdf" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="50.53125" data-pf_rect_height="19" orig-style="null"><span class="text-node">writes</span></a><span class="text-node">:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="152" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">To establish the veracity of a negated predicate in a distributed setting, an evaluation strategy has to start "counting to 0" to determine emptiness, and wait until the distributed counting process has definitely terminated. Aggregation is the generalization of this idea.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">and:</span></p>
<blockquote data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="171" orig-style="null">
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="304" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This idea can be seen from the other direction as well. Coordination protocols are themselves aggregations, since they entail voting: Two-Phase Commit requires unanimous votes, Paxos consensus requires majority votes, and Byzantine protocols require a 2/3 majority. Waiting requires counting.</span></p>
</blockquote>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">If, then we can express our computation in a manner in which it is possible to test for monotonicity, then we can perform a whole-program static analysis that detects which parts of the program are eventually consistent and safe to run without coordination (the monotonic parts) - and which parts are not (the non-monotonic ones).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Note that this requires a different kind of language, since these inferences are hard to make for traditional programming languages where sequence, selection and iteration are at the core. Which is why the Bloom language was designed.</span></p>
<h2 id="what-is-non-mononicity-good-for-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">What is non-mononicity good for?</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">The difference between monotonicity and non-monotonicity is interesting. For example, adding two numbers is monotonic, but calculating an aggregation over two nodes containing numbers is not. What's the difference? One of these is a computation (adding two numbers), while the other is an assertion (calculating an aggregate).</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">How does a computation differ from an assertion? Let's consider the query "is pizza a vegetable?". To answer that, we need to get at the core: when is it acceptable to infer that something is (or is not) true?</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">There are several acceptable answers, each corresponding to a different set of assumptions regarding the information that we have and the way we ought to act upon it - and we've come to accept different answers in different contexts.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">In everyday reasoning, we make what is known as the </span><a href="http://en.wikipedia.org/wiki/Open_world_assumption" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="189.5" data-pf_rect_height="19" orig-style="null"><span class="text-node">open-world assumption</span></a><span class="text-node">: we assume that we do not know everything, and hence cannot make conclusions from a lack of knowledge. That is, any sentence may be true, false or unknown.</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="90" class="added-to-list1" orig-style="null">                                OWA +             |  OWA +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Cannot assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Cannot assert P(true)
Cannot derive P(true)   |   Unknown               |  Unknown
or P(false)</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">When making the open world assumption, we can only safely assert something we can deduce from what is known. Our information about the world is assumed to be incomplete.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="228" class="added-to-list1" orig-style="null"><span class="text-node">Let's first look at the case where we know our reasoning is monotonic. In this case, any (potentially incomplete) knowledge that we have cannot be invalidated by learning new knowledge. So if we can infer that a sentence is true based on some deduction, such as "things that contain two tablespoons of tomato paste are vegetables" and "pizza contains two tablespoons of tomato paste", then we can conclude that "pizza is a vegetable". The same goes for if we can deduce that a sentence is false.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">However, if we cannot deduce anything - for example, the set of knowledge we have contains customer information and nothing about pizza or vegetables - then under the open world assumption we have to say that we cannot conclude anything.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">With non-monotonic knowledge, anything we know right now can potentially be invalidated. Hence, we cannot safely conclude anything, even if we can deduce true or false from what we currently know.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">However, within the database context, and within many computer science applications we prefer to make more definite conclusions. This means assuming what is known as the </span><a href="http://en.wikipedia.org/wiki/Closed_world_assumption" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="373.03125" data-pf_rect_height="38" orig-style="null"><span class="text-node">closed-world assumption</span></a><span class="text-node">: that anything that cannot be shown to be true is false. This means that no explicit declaration of falsehood is needed. In other words, the database of facts that we have is assumed to be complete (minimal), so that anything not in it can be assumed to be false.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">For example, under the CWA, if our database does not have an entry for a flight between San Francisco and Helsinki, then we can safely conclude that no such flight exists.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">We need one more thing to be able to make definite assertions: </span><a href="http://en.wikipedia.org/wiki/Circumscription_%28logic%29" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="186.46875" data-pf_rect_height="19" orig-style="null"><span class="text-node">logical circumscription</span></a><span class="text-node">. Circumscription is a formalized rule of conjecture. Domain circumscription conjectures that the known entities are all there are. We need to be able to assume that the known entities are all there are in order to reach a definite conclusion.</span></p>
<pre data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="105" class="added-to-list1" orig-style="null">                                CWA +             |  CWA +
                                Circumscription + |  Circumscription +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Can assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Can assert P(false)
Cannot derive P(true)   |   Can assert P(false)   |  Can assert P(false)
or P(false)</pre>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">In particular, non-monotonic inferences need this assumption. We can only make a confident assertion if we assume that we have complete information, since additional information may otherwise invalidate our assertion.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">What does this mean in practice? First, monotonic logic can reach definite conclusions as soon as it can derive that a sentence is true (or false). Second, nonmonotonic logic requires an additional assumption: that the known entities are all there is.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">So why are two operations that are on the surface equivalent different? Why is adding two numbers monotonic, but calculating an aggregation over two nodes not? Because the aggregation does not only calculate a sum but also asserts that it has seen all of the values. And the only way to guarantee that is to coordinate across nodes and ensure that the node performing the calculation has really seen all of the values within the system.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Thus, in order to handle nonmonotonicity one needs to either use distributed coordination to ensure that assertions are made only after all the information is known or make assertions with the caveat that the conclusion can be invalidated later on.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="152" class="added-to-list1" orig-style="null"><span class="text-node">Handling non-monotonicity is important for reasons of expressiveness. This comes down to being able to express non-monotone things; for example, it is nice to be able to say that the total of some column is X. The system must detect that this kind of computation  requires a global coordination boundary to ensure that we have seen all the entities.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">Purely monotone systems are rare. It seems that most applications operate under the closed-world assumption even when they have incomplete data, and we humans are fine with that. When a database tells you that a direct flight between San Francisco and Helsinki does not exist, you will probably treat this as "according to this database, there is no direct flight", but you do not rule out the possibility that that in reality such a flight might still exist.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">Really, this issue only becomes interesting when replicas can diverge (e.g. during a partition or due to delays during normal operation). Then there is a need for a more specific consideration: whether the answer is based on just the current node, or the totality of the system.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">Further, since nonmonotonicity is caused by making an assertion, it seems plausible that many computations can proceed for a long time and only apply coordination at the point where some result or assertion is passed to a 3rd party system or end user. Certainly it is not necessary for every single read and write operation within a system to enforce a total order, if those reads and writes are simply a part of a long running computation.</span></p>
<h2 id="the-bloom-language" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">The Bloom language</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">The </span><a href="http://www.bloom-lang.net/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="131.0625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Bloom language</span></a><span class="text-node"> is a language designed to make use of the CALM theorem. It is a Ruby DSL which has its formal basis in a temporal logic programming language called Dedalus.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="133" class="added-to-list1" orig-style="null"><span class="text-node">In Bloom, each node has a database consisting of collections and lattices. Programs are expressed as sets of unordered statements which interact with collections (sets of facts) and lattices (CRDTs). Statements are order-independent by default, but one can also write non-monotonic functions.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Have a look at the </span><a href="http://www.bloom-lang.net/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="118.953125" data-pf_rect_height="19" orig-style="null"><span class="text-node">Bloom website</span></a><span class="text-node"> and </span><a href="https://github.com/bloom-lang/bud/tree/master/docs" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="68.359375" data-pf_rect_height="19" orig-style="null"><span class="text-node">tutorials</span></a><span class="text-node"> to learn more about Bloom.</span></p>
<hr data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="2" orig-style="null">
<h2 id="further-reading" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Further reading</span></h2>
<h4 id="the-calm-theorem-confluence-analysis-and-bloom" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">The CALM theorem, confluence analysis and Bloom</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><a href="http://vimeo.com/53904989" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="293.796875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Joe Hellerstein's talk @RICON 2012</span></a><span class="text-node"> is a good introduction to the topic, as is </span><a href="http://vimeo.com/45111940" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="364.96875" data-pf_rect_height="38" orig-style="null"><span class="text-node">Neil Conway's talk @Basho</span></a><span class="text-node">. For Bloom in particular, see </span><a href="http://channel9.msdn.com/Events/Lang-NEXT/Lang-NEXT-2012/Bloom-Disorderly-Programming-for-a-Distributed-World" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="242.90625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Peter Alvaro's talk@Microsoft</span></a><span class="text-node">.</span></p>

<h4 id="crdts" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">CRDTs</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><a href="http://research.microsoft.com/apps/video/dl.aspx?id=153540" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="263.421875" data-pf_rect_height="19" orig-style="null"><span class="text-node">Marc Shapiro's talk @ Microsoft</span></a><span class="text-node"> is a good starting point for understanding CRDT's.</span></p>

<h4 id="dynamo-pbs-optimistic-replication" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Dynamo; PBS; optimistic replication</span></h4>


<h1 id="-chapter_number-further-reading-and-appendix" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null"><span class="text-node">6. Further reading and appendix</span></h1>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" class="added-to-list1" orig-style="null"><span class="text-node">If you've made it this far, thank you.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">If you liked the book, follow me on </span><a href="https://github.com/mixu/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="55.140625" data-pf_rect_height="19" orig-style="null"><span class="text-node">Github</span></a><span class="text-node"> (or </span><a href="http://twitter.com/mikitotakada" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="58.84375" data-pf_rect_height="19" orig-style="null"><span class="text-node">Twitter</span></a><span class="text-node">). I love seeing that I've had some kind of positive impact. "Create more value than you capture" and all that.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="114" class="added-to-list1" orig-style="null"><span class="text-node">Many many thanks to: logpath, alexras, globalcitizen, graue, frankshearar, roryokane, jpfuentes2, eeror, cmeiklejohn, stevenproctor eos2102 and steveloughran for their help! Of course, any mistakes and omissions that remain are my fault!</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="190" class="added-to-list1" orig-style="null"><span class="text-node">It's worth noting that my chapter on eventual consistency is fairly Berkeley-centric; I'd like to change that. I've also skipped one prominent use case for time: consistent snapshots. There are also a couple of topics which I should expand on: namely, an explicit discussion of safety and liveness properties and a more detailed discussion of consistent hashing. However, I'm off to </span><a href="https://thestrangeloop.com/" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="343.75" data-pf_rect_height="38" orig-style="null"><span class="text-node">Strange Loop 2013</span></a><span class="text-node">, so whatever.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="209" class="added-to-list1" orig-style="null"><span class="text-node">If this book had a chapter 6, it would probably be about the ways in which one can make use of and deal with large amounts of data. It seems that the most common type of "big data" computation is one in which </span><a href="http://en.wikipedia.org/wiki/SPMD" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="338.28125" data-pf_rect_height="57" orig-style="null"><span class="text-node">a large dataset is passed through a single simple program</span></a><span class="text-node">. I'm not sure what the subsequent chapters would be (perhaps high performance computing, given that the current focus has been on feasibility), but I'll probably know in a couple of years.</span></p>
<h2 id="books-about-distributed-systems" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="58" orig-style="null"><span class="text-node">Books about distributed systems</span></h2>
<h4 id="distributed-algorithms-lynch-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Distributed Algorithms (Lynch)</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="171" class="added-to-list1" orig-style="null"><span class="text-node">This is probably the most frequently recommended book on distributed algorithms. I'd also recommend it, but with a caveat. It is very comprehensive, but written for a graduate student audience, so you'll spend a lot of time reading about synchronous systems and shared memory algorithms before getting to things that are most interesting to a practitioner.</span></p>
<h4 id="introduction-to-reliable-and-secure-distributed-programming-cachin-guerraoui-rodrigues-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="57" orig-style="null"><span class="text-node">Introduction to Reliable and Secure Distributed Programming (Cachin, Guerraoui &amp; Rodrigues)</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">For a practitioner, this is a fun one. It's short and full of actual algorithm implementations.</span></p>
<h4 id="replication-theory-and-practice" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="19" orig-style="null"><span class="text-node">Replication: Theory and Practice</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" class="added-to-list1" orig-style="null"><span class="text-node">If you're interested in replication, this book is amazing. The chapter on replication is largely based on a synthesis of the interesting parts of this book plus more recent readings.</span></p>
<h4 id="distributed-systems-an-algorithmic-approach-ghosh-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">Distributed Systems: An Algorithmic Approach (Ghosh)</span></h4>
<h4 id="introduction-to-distributed-algorithms-tel-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">Introduction to Distributed Algorithms (Tel)</span></h4>
<h4 id="transactional-information-systems-theory-algorithms-and-the-practice-of-concurrency-control-and-recovery-weikum-vossen-" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="76" orig-style="null"><span class="text-node">Transactional Information Systems: Theory, Algorithms, and the Practice of Concurrency Control and Recovery (Weikum &amp; Vossen)</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">This book is on traditional transactional information systems, e.g. local RDBMS's. There are two chapters on distributed transactions at the end, but the focus of the book is on transaction processing.</span></p>
<h4 id="transaction-processing-concepts-and-techniques-by-gray-and-reuter" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" orig-style="null"><span class="text-node">Transaction Processing: Concepts and Techniques by Gray and Reuter</span></h4>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">A classic. I find that Weikum &amp; Vossen is more up to date.</span></p>
<h2 id="seminal-papers" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="29" orig-style="null"><span class="text-node">Seminal papers</span></h2>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Each year, the </span><a href="http://en.wikipedia.org/wiki/Dijkstra_Prize" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="338.53125" data-pf_rect_height="38" orig-style="null"><span class="text-node">Edsger W. Dijkstra Prize in Distributed Computing</span></a><span class="text-node"> is given to outstanding papers on the principles of distributed computing. Check out the link for the full list, which includes classics such as:</span></p>

<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="95" class="added-to-list1" orig-style="null"><span class="text-node">Microsoft Academic Search has a list of </span><a href="http://libra.msra.cn/RankList?entitytype=1&amp;topDomainID=2&amp;subDomainID=16&amp;last=0&amp;start=1&amp;end=100" data-pf_style_display="inline" data-pf_style_visibility="visible" data-pf_rect_width="353.46875" data-pf_rect_height="57" orig-style="null"><span class="text-node">top publications in distributed &amp; parallel computing ordered by number of citations</span></a><span class="text-node"> - this may be an interesting list to skim for more classics.</span></p>
<p data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="38" class="added-to-list1" orig-style="null"><span class="text-node">Here are some additional lists of recommended papers:</span></p>

<h3 id="systems" data-pf_style_display="block" data-pf_style_visibility="visible" data-pf_rect_width="384" data-pf_rect_height="22" orig-style="null"><span class="text-node">Systems</span></h3>


        </div></div></div><div class="pf-footer"></div><div id="copyright" class="non-delete"></div></div></div></div></div></div></div><form id="pf-pdf-form" method="post" action="https://pdf.printfriendly.com/pdfs/make" target="pdf_iframe" accept-charset="UTF-8">
  <input type="hidden" name="hostname" value="">
  <input type="hidden" name="code" value="">
  <input type="hidden" name="url" value="http://book.mixu.net/distsys/ebook.html">
  <input type="hidden" name="platform" value="unknown">
  <input type="hidden" name="source" value="ss">
  <input name="iehack" type="hidden" value="☠">
  <input type="hidden" name="title" value="">
  <input type="hidden" name="dir" value="">
  <input type="hidden" name="custom_css_url" value="">
</form>
<form id="pf-email-form" accept-charset="UTF-8" target="email" method="post" action="https://www.printfriendly.com/email/new">
  <input type="hidden" name="content" value="">
  <input name="iehack" type="hidden" value="☠">
  <input type="hidden" name="title" value="">
  <input type="hidden" name="url" value="">
</form>
<script>$(function() {
  pf.init();
  browserExtension.init();
});</script><footer class="container pf-ft text-sm d-flex flex-row justify-content-center align-items-center hide-for-print print-no"><a class="mx-1 mx-sm-3 pf-link hide-for-referral" href="http://support.printfriendly.com/" title="Support for PrintFriendly &amp; PDF">Support</a><a class="mx-1 mx-sm-3 pf-link hide-for-referral" href="https://www.printfriendly.com/about" title="About PrintFriendly &amp; PDF">About</a><a class="mx-1 mx-sm-3 pf-link hide-for-referral" href="https://www.printfriendly.com/terms" title="Terms of use for PrintFriendly &amp;amp; PDF">Terms</a><a class="js-pf-branding mx-1 mx-sm-3 pf-link pf-app-privacy" href="https://www.printfriendly.com/privacy" title="Privacy at PrintFriendly &amp;amp; PDF" style="display: inline;"></a><a class="pf-branding js-pf-branding mx-1 mx-sm-3 pf-link" href="https://www.printfriendly.com/" title="PrintFriendly &amp; PDF" style="display: inline;"></a></footer></div></body></html>